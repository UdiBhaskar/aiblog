[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Uday's AI Blog",
    "section": "",
    "text": "Spectrum Full Fine Tuning\n\n\n\nNLP\n\n\nPEFT\n\n\nFine Tuning\n\n\nSNR\n\n\n\nFull Fine Tuning using Spectrum (SNR)\n\n\n\nUday\n\n\nOct 30, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPEFT - Adapter Tuning\n\n\n\nNLP\n\n\nPEFT\n\n\nFine Tuning\n\n\n\nPerameter efficient finetuning using Adapters\n\n\n\nUday\n\n\nSep 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary - Logistic Regression Algorithm\n\n\n\nLogistic Regression\n\n\nclassification\n\n\n\nsummary and useful points to know about LR\n\n\n\nUday\n\n\nAug 11, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary - KNN Algorithm\n\n\n\nKNN\n\n\n\nTraining process and useful points to know about KNN\n\n\n\nUday\n\n\nAug 10, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNormalization for Better Generalization and Faster Training\n\n\n\nNLP\n\n\nBatchnorm\n\n\nlayernorm\n\n\nnormalization\n\n\n\nDifferent types of Normalization layers ( Batch Norm, Layernorm)\n\n\n\nUday\n\n\nJun 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHypothesis Testing\n\n\n\nHypothesis testing\n\n\nnull hypothesis\n\n\n2-sample test\n\n\none-sample-test\n\n\np-value\n\n\nanova\n\n\n\nHypothesis testing using permutation\n\n\n\nUday\n\n\nJun 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFoundations to inference Statistics\n\n\n\nCLT\n\n\nsampling\n\n\nz-distribution\n\n\nt-distribution\n\n\nCI\n\n\nConfidence Interval\n\n\nBootstrapping\n\n\n\nBasics of inference statistics, sampling distributions, Bootstrapping, Confidence Intervals\n\n\n\nUday\n\n\nJun 7, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced Feature Extraction from Text\n\n\n\nNLP\n\n\nfeature extraction\n\n\nword2vec\n\n\nfasttext\n\n\n\nDense vector features for text\n\n\n\nUday\n\n\nMar 16, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEffective Training and Debugging of a Neural Networks\n\n\n\nDebugging NN\n\n\nNeural Network\n\n\ntraining\n\n\n\nProper ways to training and debugging of a neural network\n\n\n\nUday\n\n\nFeb 3, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notebooks/Foundations to inference Stats.html",
    "href": "notebooks/Foundations to inference Stats.html",
    "title": "Foundations to inference Statistics",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import rcParams\n# figure size in inches\nrcParams['figure.figsize'] = 11.7,8.27"
  },
  {
    "objectID": "notebooks/Foundations to inference Stats.html#sampling-distribution",
    "href": "notebooks/Foundations to inference Stats.html#sampling-distribution",
    "title": "Foundations to inference Statistics",
    "section": "Sampling Distribution",
    "text": "Sampling Distribution\n\n\n\n‘title’\n\n\nSteps for the sampling distribution\n\nGet n random samples each of size m\nCalculate the sample statistic for each random sample.\nplot the sample statistics distribution\n\n\nnp.random.seed(10)\nrandom_population = np.random.beta(4, 5, size=1000)\nrandom_sample_population = np.random.choice(random_population, 400, False) #\n\nsns.distplot(random_sample_population, hist=True, kde=True, color='red', norm_hist=True, label=\"Population Dist\")\nplt.title('Population')\nplt.legend()\n\n\n\n\n\n\n\n\n\nprint('random population Mean --&gt;', np.mean(random_sample_population))\nprint('random population SD --&gt;', np.std(random_sample_population))\n\nrandom population Mean --&gt; 0.4597295869346082\nrandom population SD --&gt; 0.16002188241803594\n\n\n\nCode to get sampling distribution\n\nno_of_samples = 5000\nsample_size = 50\ndef get_sampling_dist(population, no_of_samples, sample_size, stat):\n    '''\n    population - random sample population we have \n               - array or list\n    no_of_samples - number of samples (n)\n    sample_size - size of each sample.(m) \n    stat - sample stat to calculate. \n        - Function\n    '''\n    sampling_dist = []\n    for i in range(no_of_samples): ## n samples\n        sample = np.random.choice(population, sample_size, False) # each of size m\n        stat_val = stat(sample) #calculating stat\n        sampling_dist.append(stat_val)\n    return sampling_dist\n\nsampling_dist_mean = get_sampling_dist(random_sample_population, no_of_samples, sample_size, np.mean)\n\n\n##ploting the sampling distribution. \nsns.distplot(sampling_dist_mean, hist=True, kde=True, color='red', norm_hist=True, label=\"Sampling dist of mean\")\nplt.title('Sampling Dist of Mean')\nplt.xlabel('sample means')\nplt.legend()\n\n\n\n\n\n\n\n\n\nprint('Sampling Dist Mean --&gt;', np.mean(sampling_dist_mean))\nprint('Sampling Dist SD --&gt;', np.std(sampling_dist_mean))\n\nSampling Dist Mean --&gt; 0.46022309784271165\nSampling Dist SD --&gt; 0.020887127365585376\n\n\n\nWe can do the above sampling distribution for any statistic value.\nIf we observe the above means, sampling dist mean(x_bar) is nearly equal to the population mean(mu)\nWe will call std of sampling dist as standard error"
  },
  {
    "objectID": "notebooks/Foundations to inference Stats.html#central-limit-theorem",
    "href": "notebooks/Foundations to inference Stats.html#central-limit-theorem",
    "title": "Foundations to inference Statistics",
    "section": "Central Limit Theorem:",
    "text": "Central Limit Theorem:\nThe distribution of sample means is nearly normal with mean = poluation mean, std = population_std/sqrt(sample_size).\n\n\\[\\begin{align}\n  \\text{Sampling Dist ~}  N( \\mu, \\frac{\\sigma}{\\sqrt{\\text{sample size}}}) \\\\\n    \\end{align}\\]\n\n\nConditions\n\nSampled observations must be independent.\nIf we do sampling with replacement, the sample size must be less than 10% of the population.\nThe sample size of 30 is sufficient for most distributions. However, strongly skewed distributions can require larger sample sizes.\n\n\n\nTip: We can simulate the CLT with https://gallery.shinyapps.io/CLT_mean/\n\n\nprint(0.020887127365585376*np.sqrt(sample_size))\nprint(np.std(random_population))\n\n0.14769429399712528\n0.15723732430046308\n\n\n\nWarning: Can the Central Limit theorem apply to any other sample statistic like median, std? – No\n\n\n\nNeed for Confidence Interval\nThere will be variability in the point estimate because we can’t get the exact population data in real-time. so if we tell a range of mean(any stat), it will be useful. This is called a Confidence Interval. Before going into CI, we have to know Z and t distribution, confidence level."
  },
  {
    "objectID": "notebooks/Foundations to inference Stats.html#z--distribution-or-standard-normal-distribution",
    "href": "notebooks/Foundations to inference Stats.html#z--distribution-or-standard-normal-distribution",
    "title": "Foundations to inference Statistics",
    "section": "Z- Distribution or standard normal distribution:",
    "text": "Z- Distribution or standard normal distribution:\n\n\\[\\begin{align}\n  Z =  \\frac{ x - \\mu}{\\sigma} \\\\\n    \\end{align}\\]\n\n\nNote: mean = 0, std = 1\n\n\nstandard_normal = np.random.standard_normal(size=100000)\nsns.distplot(standard_normal, hist=True, kde=True, color='red', norm_hist=True)\nplt.title('Z-Distribution')\n\nText(0.5, 1.0, 'Z-Distribution')"
  },
  {
    "objectID": "notebooks/Foundations to inference Stats.html#confidence-level",
    "href": "notebooks/Foundations to inference Stats.html#confidence-level",
    "title": "Foundations to inference Statistics",
    "section": "Confidence level",
    "text": "Confidence level\nThe probability that the value of a parameter falls within a specified range of values. This will be very useful when we tell an interval. We can tell like, with 95% confidence, our statistic/parameter lies between lower bound and upper bound.\n\nLet’s calculate confidence levels for the above z distribution.\nFor 95% confidence, (100-95)/2 = 2.5, we have to get the 2.5 percentile and (100-2.5)=97.5 percentile.\n\nnp.percentile(standard_normal, [2.5, 97.5])\n\narray([-1.94948191,  1.94622294])\n\n\nOur z scores lie between -1.95 and 1.95 with 95% confidence. based on above simulation( right value is 1.96, if we take more size, we will get 1.96) We can get this using (100-C)/2, 100-((100-c)/2) where C = 95 if we need 95% of confidence.\nWe can get these using the scipy.stats.norm.ppf function but in this function takes all the values with 0-1 only not 0-100 so 95% will become 0.95. We can get this using (1-C)/2, 1-((1-c)/2) where C = 0.95 if we need 95% of confidence.\n\nfrom scipy.stats import norm\ndef get_qnorm(CL):\n    '''get the  value in zdist for given CL\n    CL - Confidence level(0-1)'''\n    val = (1-CL)/2\n    return norm.ppf(val), norm.ppf(1-val)\nget_qnorm(0.95)\n\n(-1.959963984540054, 1.959963984540054)\n\n\n\n##99 confidence level\nget_qnorm(0.99)\n\n(-2.5758293035489004, 2.5758293035489004)\n\n\n\n##64 confidence level\nget_qnorm(0.684)\n\n(-1.0027116650265495, 1.0027116650265495)"
  },
  {
    "objectID": "notebooks/Foundations to inference Stats.html#students-t-distribution",
    "href": "notebooks/Foundations to inference Stats.html#students-t-distribution",
    "title": "Foundations to inference Statistics",
    "section": "Student’s t-distribution:",
    "text": "Student’s t-distribution:\nIt is useful when population std is unknown. If the sample size is small, we may not get better results with the Z distribution. It is similar to the Z distribution bell-shaped curve but thicker tails than normal. Other than mean, std, it has another parameter called degree of freedom = n-1. It is wider so intervals that we get from t-dist are also wider.\n\n\n\n‘title’\n\n\n\nfrom scipy.stats import t\ndef get_qnorm_t(CL, df):\n    '''get the  value in t-dist for given CL\n    CL - Confidence level(0-1)'''\n    val = (1-CL)/2\n    return t.ppf(val, df), t.ppf(1-val, df)\nget_qnorm_t(0.95, 29)\n\n(-2.045229642132703, 2.045229642132703)\n\n\n\nDifference between t-dist and z-dist\nIf we have more degrees of freedom(more samples), t-distribution will look like z-distribution. You can check that below.\n\nfor i in range(0, 1000, 50):\n    print('95% of CL with df '+str(i)+' is --&gt;',get_qnorm_t(0.95, i))\n\n95% of CL with df 0 is --&gt; (nan, nan)\n95% of CL with df 50 is --&gt; (-2.008559109715206, 2.008559109715206)\n95% of CL with df 100 is --&gt; (-1.9839715184496334, 1.9839715184496334)\n95% of CL with df 150 is --&gt; (-1.9759053308869137, 1.9759053308869137)\n95% of CL with df 200 is --&gt; (-1.9718962236316089, 1.9718962236316089)\n95% of CL with df 250 is --&gt; (-1.9694983934204002, 1.9694983934204002)\n95% of CL with df 300 is --&gt; (-1.9679030112607843, 1.9679030112607843)\n95% of CL with df 350 is --&gt; (-1.9667650028635124, 1.9667650028635124)\n95% of CL with df 400 is --&gt; (-1.965912343229391, 1.965912343229391)\n95% of CL with df 450 is --&gt; (-1.965249664736427, 1.965249664736427)\n95% of CL with df 500 is --&gt; (-1.9647198374673438, 1.9647198374673438)\n95% of CL with df 550 is --&gt; (-1.964286550912067, 1.964286550912067)\n95% of CL with df 600 is --&gt; (-1.9639256220427195, 1.9639256220427195)\n95% of CL with df 650 is --&gt; (-1.963620322372358, 1.963620322372358)\n95% of CL with df 700 is --&gt; (-1.963358711099814, 1.963358711099814)\n95% of CL with df 750 is --&gt; (-1.9631320366857694, 1.9631320366857694)\n95% of CL with df 800 is --&gt; (-1.9629337387277888, 1.9629337387277888)\n95% of CL with df 850 is --&gt; (-1.9627588026071148, 1.9627588026071148)\n95% of CL with df 900 is --&gt; (-1.9626033295371894, 1.9626033295371894)\n95% of CL with df 950 is --&gt; (-1.962464242556152, 1.962464242556152)\n\n\n\nget_qnorm(0.95)\n\n(-1.959963984540054, 1.959963984540054)\n\n\n\nget_qnorm_t(0.95, 50)\n\n\nfor i in range(0, 1000, 50):\n    print('90% of CL with df '+str(i)+' is --&gt;',get_qnorm_t(0.90, i))\nprint('-'*50)\nprint('90% of CL in Z-dist is --&gt;', get_qnorm(0.90))\n\n90% of CL with df 0 is --&gt; (nan, nan)\n90% of CL with df 50 is --&gt; (-1.6759050245283318, 1.6759050245283311)\n90% of CL with df 100 is --&gt; (-1.6602343260657506, 1.66023432606575)\n90% of CL with df 150 is --&gt; (-1.655075500184607, 1.6550755001846063)\n90% of CL with df 200 is --&gt; (-1.6525081009102696, 1.652508100910269)\n90% of CL with df 250 is --&gt; (-1.6509714898126593, 1.6509714898126586)\n90% of CL with df 300 is --&gt; (-1.6499486739375542, 1.6499486739375535)\n90% of CL with df 350 is --&gt; (-1.6492188695371959, 1.6492188695371952)\n90% of CL with df 400 is --&gt; (-1.6486719414653956, 1.648671941465395)\n90% of CL with df 450 is --&gt; (-1.6482468047587875, 1.6482468047587868)\n90% of CL with df 500 is --&gt; (-1.6479068539295052, 1.6479068539295045)\n90% of CL with df 550 is --&gt; (-1.6476288171096811, 1.6476288171096805)\n90% of CL with df 600 is --&gt; (-1.647397191759995, 1.6473971917599943)\n90% of CL with df 650 is --&gt; (-1.6472012521875499, 1.6472012521875492)\n90% of CL with df 700 is --&gt; (-1.6470333412605698, 1.647033341260569)\n90% of CL with df 750 is --&gt; (-1.6468878462849894, 1.6468878462849887)\n90% of CL with df 800 is --&gt; (-1.6467605593740848, 1.6467605593740842)\n90% of CL with df 850 is --&gt; (-1.6466482638172075, 1.6466482638172069)\n90% of CL with df 900 is --&gt; (-1.6465484584682117, 1.646548458468211)\n90% of CL with df 950 is --&gt; (-1.6464591692544057, 1.646459169254405)\n--------------------------------------------------\n90% of CL in Z-dist is --&gt; (-1.6448536269514729, 1.6448536269514722)\n\n\nFrom above, we can observe that, if df is large value( i.e n is large), t-distribution will yield similar results as z distribution."
  },
  {
    "objectID": "notebooks/Foundations to inference Stats.html#confidence-interval-for-population-mean",
    "href": "notebooks/Foundations to inference Stats.html#confidence-interval-for-population-mean",
    "title": "Foundations to inference Statistics",
    "section": "Confidence Interval for Population Mean:",
    "text": "Confidence Interval for Population Mean:\nWhy we need CI:\nThere will be variability in the point estimate because we can’t get the exact population data in real-time. so if we tell a range of mean, it will be useful.\n\nFrom the above CLT theorem, we know that sampling means follows a Normal distribution. we also know the properties of standard normal distribution like 68-95-99.7 rules( even we can compute for any value) i.e we can tell with 68% confidence that, mean is between mean-1*std_sampling_dist, mean+1*std_sampling_dist.\n\n\\[\\begin{align}\n  \\text{CI of Mean =}  \\mu \\pm z^{*} * SE\\\\\n  \\text{CI of Mean =}  \\mu \\pm t^{*} * SE\\\\\n    \\end{align}\\]\n\nfrom scipy.stats import norm\ndef get_ci_mean(sampling_mean, SE, ci):\n    '''Get CI for mean using z-dist\n    sampling_mean - sample mean\n    SE - Standard error from CLT\n    CI - Confidence level'''\n    z_temp = (1-(ci/100))/2\n    z = abs(norm.ppf(z_temp))\n    lower_bound = sampling_mean - z * SE\n    upper_bound = sampling_mean + z * SE\n    return lower_bound, upper_bound\n\n\nn5_ci = get_ci_mean(np.mean(sampling_dist_mean), np.std(sampling_dist_mean), 95)\nprint('95% CI is', n5_ci)\n\n95% CI is (0.4192850804656633, 0.5011611152197599)\n\n\n\nfrom scipy.stats import t\ndef get_ci_mean_t(sampling_mean, SE, ci, df):\n    '''Get CI for mean using t-dist\n    sampling_mean - sample mean\n    SE - Standard error from CLT\n    CI - Confidence level\n    df - degrees of freedom, (n-1)'''\n    t_temp = (1-(ci/100))/2\n    t_val = abs(t.ppf(t_temp, df))\n    lower_bound = sampling_mean - t_val * SE\n    upper_bound = sampling_mean + t_val * SE\n    return lower_bound, upper_bound\n\n\nn5_ci_t = get_ci_mean_t(np.mean(sampling_dist_mean), np.std(sampling_dist_mean), 95, sample_size-1)\nprint('95% CI is using t', n5_ci_t)\n\n95% CI is using t (0.41824884396920947, 0.5021973517162138)\n\n\n\nlen(sampling_dist_mean)\n\n5000\n\n\n\nTip: t distribution CI is wider than the Z distribution. Many times we don’t know what is the std of the population so it is better to use t distribution than z. If the sample size is larger, we can go for Z distribution(no issues if we have a large sample to analysis)."
  },
  {
    "objectID": "notebooks/Foundations to inference Stats.html#how-to-predict-ci-parameters-other-than-mean",
    "href": "notebooks/Foundations to inference Stats.html#how-to-predict-ci-parameters-other-than-mean",
    "title": "Foundations to inference Statistics",
    "section": "How to predict CI parameters other than mean",
    "text": "How to predict CI parameters other than mean\nWe know about sampling mean distribution so we can CI of mean very easily but how can we predict for median or percentile or IQR?\n\nThere is another set of methods to do stats called Non-Parametric methods. We can use the non-parametric methods and get the CI for any value without knowing the underlying distribution."
  },
  {
    "objectID": "notebooks/Foundations to inference Stats.html#bootstrapping",
    "href": "notebooks/Foundations to inference Stats.html#bootstrapping",
    "title": "Foundations to inference Statistics",
    "section": "Bootstrapping:",
    "text": "Bootstrapping:\nBootstrap Sample is Sampling with replacement of data of same size as shown below\n\n\n\n‘title’\n\n\n\nWhy same size?\nthe variation of the statistic will depend on the size of the sample. If we want to approximate this variation we need to use resamples of the same size.\n\n\nUsing Python\nWe can use np.random.choice to get the bootstrap samples.\n\n###code to generate a Bootstrap sample\ntemp_population = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\nprint('Random Bootstrap sample from temp_population --&gt;', \n      np.random.choice(temp_population, size=len(temp_population),replace=True))\nprint('Random Bootstrap sample from temp_population --&gt;', \n      np.random.choice(temp_population, size=len(temp_population),replace=True))\nprint('Random Bootstrap sample from temp_population --&gt;', \n      np.random.choice(temp_population, size=len(temp_population),replace=True))\n\nRandom Bootstrap sample from temp_population --&gt; [ 3 10  1  8  5  8  8  3  6  5]\nRandom Bootstrap sample from temp_population --&gt; [3 8 5 4 4 1 9 9 3 6]\nRandom Bootstrap sample from temp_population --&gt; [10 10  4  7  2 10  9  2  7  3]"
  },
  {
    "objectID": "notebooks/Foundations to inference Stats.html#ci-using-bootstrapping",
    "href": "notebooks/Foundations to inference Stats.html#ci-using-bootstrapping",
    "title": "Foundations to inference Statistics",
    "section": "CI using Bootstrapping:",
    "text": "CI using Bootstrapping:\nThere are many ways to calculate bootstrap samples.\n\nPercentile Method\nBasic bootstrap Method or Reverse Percentile Interval\nStudentized-t Bootstrap Method\nBias Corrected and accelerated\n\n\nSome notations:\n\\(\\tilde{\\theta}_{m}\\) - statistic of Bootstrap sample m\n\\(\\hat{\\theta}\\) - statistic of data we have.\n\\(\\theta\\) - statistic what we want to estimate.\n\nPercentile Method:\nsteps:\n\nGet the m bootstrap samples.\ncalculate the m statistics \\(\\tilde{\\theta}_{1}, \\tilde{\\theta}_{1}, \\tilde{\\theta}_{3} ... \\tilde{\\theta}_{m}\\)\nGet the percentile values based in the how much confidance we need. Eg. for 95% CL, get 2.5 percentile and 97.5 percentile.\n\n\nCode\n\nnp.random.seed(10)\ndef get_percentile_bs_ci(data, m, stat, cl):\n    '''\n    percentile method to get BS-CI\n    data - data we have(sample)\n    m - number of bootstrap samples\n    stat - statistic to find - a function\n    cl - confidence level\n    '''\n    theta_stat = []\n    for i in range(m):\n        bs_sample = np.random.choice(data, m)\n        theta_stat.append(stat(bs_sample))\n    sns.distplot(theta_stat, hist=True, kde=True, color='red', norm_hist=True)\n    lower_bound = (1-cl)/2\n    upper_bound = 1-lower_bound\n    return np.percentile(theta_stat, [lower_bound*100, upper_bound*100])\n\n\n##using above function to get bootstrap CI\nget_percentile_bs_ci(random_sample_population, 10000, np.mean, 0.95)\n\narray([0.45656996, 0.46282595])\n\n\n\n\n\n\n\n\n\nWe can ge the same thing from arch module \n\nfrom arch.bootstrap import IIDBootstrap\nbs = IIDBootstrap(random_sample_population)\n\n\nbs.conf_int(func=np.mean, reps=10000, method='percentile', size=0.95)\n\narray([[0.44397996],\n       [0.47526665]])\n\n\n\nImportant:CI from the percentile method is very narrow. If the underlying distribution is skew, it won’t work properly. so go for basic method or studentized-t method.\n\n\n\n\nBasic Bootstrap Method or Reverse Percentile Interval Method:\n\n\n\n‘Reverse Percentile Interval’\n\n\nsteps:\n\ncalculate the statistic \\(\\hat{\\theta}\\) on data we have.\nGet the m bootstrap samples.\ncalculate the m statistics \\(\\tilde{\\theta}_{1}, \\tilde{\\theta}_{1}, \\tilde{\\theta}_{3} ... \\tilde{\\theta}_{m}\\)\nCalculate the CI with above formula.\n\n\nNote: It assumes distribution of \\(\\hat{\\theta}-\\tilde{\\theta}\\) and \\(\\theta-\\hat{\\theta}\\) are simialr\n\n\nCode\n\nnp.random.seed(10)\ndef get_basic_bs_ci(data, m, stat, cl):\n    '''\n    Reverse Percentile Interval Method\n    data- sample we have\n    m - number of bootstrap samples\n    stat - stat function to calculate\n    cl - confidence level\n    '''\n    hat_theta = stat(data)\n    theta_stat = []\n    for i in range(m):\n        bs_sample = np.random.choice(data, m)\n        theta_stat.append(stat(bs_sample))\n    #sns.distplot(theta_stat, hist=True, kde=True, color='red', norm_hist=True)\n    lower_bound = (1-cl)/2\n    upper_bound = 1-lower_bound\n    lower_bound1 = 2*hat_theta - np.percentile(theta_stat, upper_bound*100)\n    upper_bound1 = 2*hat_theta - np.percentile(theta_stat, lower_bound*100)\n    return lower_bound1, upper_bound1\n\n\nget_basic_bs_ci(random_sample_population, 1000, np.mean, 0.95)\n\n(0.44972018316026047, 0.47056730107433303)\n\n\nYou can get the more optimized code from arch.bootstrap\n\nfrom arch.bootstrap import IIDBootstrap\nbs = IIDBootstrap(random_sample_population)\n\n\nbs.conf_int(func=np.mean, reps=10000, method='basic', size=0.95)\n\narray([[0.44406911],\n       [0.47499762]])\n\n\n\n\n\nStudentized-t Bootstrap Method:\nIf the distributions of \\(\\hat{\\theta}-\\tilde{\\theta}\\) and \\(\\theta-\\hat{\\theta}\\) are not close, then the basic bootstrap confidence interval can be inaccurate. But even in this case, the distributions of \\(\\frac{\\hat{\\theta}-\\tilde{\\theta}}{SE(\\tilde{\\theta})}\\) and \\(\\frac{\\theta-\\hat{\\theta}}{SE({\\hat{\\theta}})}\\) could be close. hence we could use studentized bootstrap CI. \n\n\n\n‘studentized bs’\n\n\n\nsteps:\n\ncalculate the statistic \\(\\hat{\\theta}\\) on data we have.\nGet the m bootstrap samples.\nFor each Bootstrap sample\n\ncompute the \\(\\tilde{\\theta}\\)\ncompute \\(SE({\\tilde{\\theta}})\\)\ncompute \\(q = \\frac{\\tilde{\\theta}-\\hat{\\theta}}{SE(\\tilde{\\theta})}\\)\n\nEstimate \\(SE(\\hat{\\theta})\\) (You can directly compute or use another Bootstrap approach.)\nCalculate CI using above formulation.\n\n\ncode\n\nfrom arch.bootstrap import IIDBootstrap\nbs = IIDBootstrap(random_sample_population)\n\n\nbs.conf_int(func=np.mean, reps=1000, method='studentized', size=0.95)\n\narray([[0.44364941],\n       [0.47571528]])\n\n\n\n\n\nBias Corrected and accelerated bootstrap CI:\nThe main advantage to the BCa interval is that it corrects for bias and skewness in the distribution of bootstrap estimates. The BCa interval requires that you estimate two parameters. The bias-correction parameter, z0, is related to the proportion of bootstrap estimates that are less than the observed statistic. The acceleration parameter, a, is proportional to the skewness of the bootstrap distribution\nTo compute a BCa confidence interval, you estimate z0 and a and use them to adjust the endpoints of the percentile confidence interval (CI). If the bootstrap distribution is positively skewed, the CI is adjusted to the right. If the bootstrap distribution is negatively skewed, the CI is adjusted to the left.\nIf the statistic is biased upward (that is, if it tends to be too large), the BCa bias correction moves the endpoints to the left. If the bootstrap distribution is skewed to the right, the BCa incorporates a correction to move the endpoints even farther to the right.\nYou can read more about calculating z0 and a in https://projecteuclid.org/download/pdf_1/euclid.ss/1032280214\n\ncode\n\nfrom arch.bootstrap import IIDBootstrap\nbs = IIDBootstrap(random_sample_population)\n\n\nbs.conf_int(func=np.mean, reps=1000, method='bca', size=0.95)\n\narray([[0.44443402],\n       [0.47456652]])"
  },
  {
    "objectID": "notebooks/Foundations to inference Stats.html#when-to-use-what",
    "href": "notebooks/Foundations to inference Stats.html#when-to-use-what",
    "title": "Foundations to inference Statistics",
    "section": "When to use what?",
    "text": "When to use what?\n\nIf we have small sample size, basic methods like “basic”, “percentile” may give wider intervels so ‘BCa’ or ‘Studentized-t’ may be better. If you have skewness, go for ‘BCa’.\nIf we have large data, all methods may give better intervals but if we have skewness, it is better to go for ‘BCa’.\n\n\n\n\n\n‘when to use’"
  },
  {
    "objectID": "notebooks/Normalization.html",
    "href": "notebooks/Normalization.html",
    "title": "Normalization for Better Generalization and Faster Training",
    "section": "",
    "text": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization and makes it notoriously hard to train models with saturating nonlinearities. so to overcome this, we can do a normalization after some layers as below.\n\n\n\n\nBN\n\n\n\nIt calculates the batch means, std, and using those, normalizes the data then creates running mean and std which will be used in inference. One intuition about why BatchNorm works is that it removes the internal covariance shift. You can check that in the below video.\n\n\nyoutube: https://www.youtube.com/watch?v=nUUqwaxLnWs\n\n\nAnother intuition:\nBatch Normalization normalizes the activations in the intermediate layers. BN primarily enables training with a larger learning rate which is cause for faster convergence and better generalization.\nLarger batch size training may converge to sharp minima. If we converge to sharp minima, generalization capacity may decrease. so noise in the SGD has an important role in regularizing the NN. Similarly, Higher learning rate will bias the network towards wider minima so it will give the better generalization. But, training with a higher learning rate may cause an explosion in the updates.\nIf we compare the gradients between with batch normalization and without batch normalization, without batch norm network gradients are larger and heavier tailed as shown below so we can train with larger learning rates with BN.\n\n\n\nBN\n\n\n\n\nImportant: BN is widely adopted in computer vision but, it leads to significant performance degradation for NLP. Nowadays Layer Normalization is preferred normalization technique for NLP tasks.\n\n\n\nNote: BN cannot be applied to online learning tasks. BN cannot applied to extremely large distributed models where the minibatches have to be small. For forward neural networks, BN can be directly applied, because each layer has a fixed number of neurons, and the mean and variance statistics of each neuron in each layer of the network can be directly stored for model prediction, but in the RNNs network, different mini-batch may have different input sequence length, it is difficult to calculate statistical information, and the test sequence length cannot be greater than the maximum training sequence length\n\n\nYou can check the figure below from a paper, which compares the BN in CV and NLP. The differences between running mean/Variance and batch mean/variance exhibit very high variance with extreme outliers in Transformers.\n\n\n\nBN\n\n\n\nimport tensorflow as tf\ninput_layer = tf.keras.Input(shape=(6,))\nbn_layer = tf.keras.layers.BatchNormalization()\nbn_layer_out = bn_layer(input_layer)\nprint('Number of weights is', len(bn_layer.get_weights()))\n\nNumber of weights is 4\n\n\nIf we have n features as input to the BN layer, the weight matrix we have to learn is of size (4, n), i.e. n features for each beta_initializer, gamma_initializer, moving_mean_initializer, moving_variance_initializer. Please read Tensorflow documentation to know more about Training mode, inference mode of the BN layer. It is very important to take care of the mode in BN layer."
  },
  {
    "objectID": "notebooks/Normalization.html#batch-normalization",
    "href": "notebooks/Normalization.html#batch-normalization",
    "title": "Normalization for Better Generalization and Faster Training",
    "section": "",
    "text": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization and makes it notoriously hard to train models with saturating nonlinearities. so to overcome this, we can do a normalization after some layers as below.\n\n\n\n\nBN\n\n\n\nIt calculates the batch means, std, and using those, normalizes the data then creates running mean and std which will be used in inference. One intuition about why BatchNorm works is that it removes the internal covariance shift. You can check that in the below video.\n\n\nyoutube: https://www.youtube.com/watch?v=nUUqwaxLnWs\n\n\nAnother intuition:\nBatch Normalization normalizes the activations in the intermediate layers. BN primarily enables training with a larger learning rate which is cause for faster convergence and better generalization.\nLarger batch size training may converge to sharp minima. If we converge to sharp minima, generalization capacity may decrease. so noise in the SGD has an important role in regularizing the NN. Similarly, Higher learning rate will bias the network towards wider minima so it will give the better generalization. But, training with a higher learning rate may cause an explosion in the updates.\nIf we compare the gradients between with batch normalization and without batch normalization, without batch norm network gradients are larger and heavier tailed as shown below so we can train with larger learning rates with BN.\n\n\n\nBN\n\n\n\n\nImportant: BN is widely adopted in computer vision but, it leads to significant performance degradation for NLP. Nowadays Layer Normalization is preferred normalization technique for NLP tasks.\n\n\n\nNote: BN cannot be applied to online learning tasks. BN cannot applied to extremely large distributed models where the minibatches have to be small. For forward neural networks, BN can be directly applied, because each layer has a fixed number of neurons, and the mean and variance statistics of each neuron in each layer of the network can be directly stored for model prediction, but in the RNNs network, different mini-batch may have different input sequence length, it is difficult to calculate statistical information, and the test sequence length cannot be greater than the maximum training sequence length\n\n\nYou can check the figure below from a paper, which compares the BN in CV and NLP. The differences between running mean/Variance and batch mean/variance exhibit very high variance with extreme outliers in Transformers.\n\n\n\nBN\n\n\n\nimport tensorflow as tf\ninput_layer = tf.keras.Input(shape=(6,))\nbn_layer = tf.keras.layers.BatchNormalization()\nbn_layer_out = bn_layer(input_layer)\nprint('Number of weights is', len(bn_layer.get_weights()))\n\nNumber of weights is 4\n\n\nIf we have n features as input to the BN layer, the weight matrix we have to learn is of size (4, n), i.e. n features for each beta_initializer, gamma_initializer, moving_mean_initializer, moving_variance_initializer. Please read Tensorflow documentation to know more about Training mode, inference mode of the BN layer. It is very important to take care of the mode in BN layer."
  },
  {
    "objectID": "notebooks/Normalization.html#layer-normalization",
    "href": "notebooks/Normalization.html#layer-normalization",
    "title": "Normalization for Better Generalization and Faster Training",
    "section": "Layer Normalization",
    "text": "Layer Normalization\nUnlike Batch normalization, it normalized horizontally i.e. it normalizes each data point. so \\(\\mu\\), \\(\\sigma\\) not depend on the batch. layer normalization does not have to use “running mean” and “running variance”.\n\n\n\nlayernorm\n\n\nIt gives the better results because of the gradinets with respect to \\(\\mu\\), \\(\\sigma\\) in Layer Normalization. Derivative of \\(\\mu\\) re-centers network gradients to zero. Derivative of \\(\\sigma\\) reduces variance of network gradient, which can be seen a kind of re-scaling.\n\n\nImportant: The parameters of LayerNorm, including the bias and gain, increase the risk of over-fitting, and do not work in most cases. - https://papers.nips.cc/paper/8689-understanding-and-improving-layer-normalization.pdf. You can remove these using center, scale parameters in Tensorflow.\n\n\nimport tensorflow as tf\ninput_layer = tf.keras.Input(shape=(6))\nnorm_layer = tf.keras.layers.LayerNormalization(scale=False, center=False)\nnorm_layer_out = norm_layer(input_layer)\nprint('Number of weights is', len(norm_layer.get_weights()))\n\nNumber of weights is 0\n\n\n\nNote: If there is no gain and bias, number of weights is zero.\n\n\nimport tensorflow as tf\ninput_layer = tf.keras.Input(shape=(10,),batch_size=1)\nnorm_layer = tf.keras.layers.LayerNormalization(scale=True, center=True)\nnorm_layer_out = norm_layer(input_layer)\nprint('Number of weights is', len(norm_layer.get_weights()))\n\nNumber of weights is 2"
  },
  {
    "objectID": "notebooks/Hypothesis-Testing.html",
    "href": "notebooks/Hypothesis-Testing.html",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "We start with a null hypothesis (H0) that represents the currect state ( Nothing going on, =, &lt;=, &gt;=)\nWe also have an alternative hypothesis(HA) that represents the research question we are testing. ( something is hoing on, !=, &gt;, &lt;)\nWe conduct a hypothesis test under the assumption that null hypothesis is true, either via simulation(Permutation test) or theoretical test using CLT.\n\nChoose a test statistic\nCompute the test statistic.\nDetermine the frequency distribution of the test statistic under the hypothesis.\nMake a decision using this distribution as a guide as discussed in the point-4.\n\nIf the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, we stick with the null hypothesis. If they do, then we reject the null hypothesis in the favor of the alternate hypothesis. If P value is low (Lower than significance level), we say that it is unlikely to observe the data if the null hypotheis were true, and hence reject the null hypothesis. If it is High, we won’t reject the null hypothesis.\n\nAlways construct hypotheses about population parameters (e.g. population mean, μ) and not the sample statistics (e.g. sample mean)\n\nNote that the alternative hypothesis might be one-sided (μ &lt; or &gt; the null value) or two-sided (μ≠ the null value), and the choice depends on the research question.\n\n\n\nImportant: P Value is the probability of obtaining a value of your test statistic that is at least as extream as what ws observed, under the assumption the null hypothesis is true. It is not the probability that the null hypothesis is True.\n\n\n\nNote: P Value = Conditional probability of data given null hypothesis is true = P(observed or more extreme sample statistic ∣ H0 true)\n\n\n\n\nAnalyze the problem and state the Null Hypothesis\nState the Alternate Hypothesis\nChoose a test statistic and Compute the test statistic\nDetermine the frequency distribution of the test statistic under the hypothesis.\nCalculate the P-Value for your CL based on two tail/single tail.\nMake a decision.\n\n\nYou can do one tailed or two tailed test based on the alternate hypothesis.\n\nA two sided Hypothesis with significance level alpha is equivalent to a confidence interval with \\(CL = 1 - \\alpha\\)\nA one sided Hypothesis with significance level alpha is equivalent to a confidence interval with \\(CL = 1- (2*\\alpha)\\)\n\n\n\n\n‘hypothesis cl’\n\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\n# figure size in inches\nrcParams['figure.figsize'] = 11.7,8.27\n\n\n#you can download the data from https://www.kaggle.com/spscientist/students-performance-in-exams\nsample_data = pd.read_csv(\"datasets_74977_169835_StudentsPerformance.csv\")\n\n\n##sample data \nsample_data.head()\n\n\n\n\n\n\n\n\ngender\nrace/ethnicity\nparental level of education\nlunch\ntest preparation course\nmath score\nreading score\nwriting score\n\n\n\n\n0\nfemale\ngroup B\nbachelor's degree\nstandard\nnone\n72\n72\n74\n\n\n1\nfemale\ngroup C\nsome college\nstandard\ncompleted\n69\n90\n88\n\n\n2\nfemale\ngroup B\nmaster's degree\nstandard\nnone\n90\n95\n93\n\n\n3\nmale\ngroup A\nassociate's degree\nfree/reduced\nnone\n47\n57\n44\n\n\n4\nmale\ngroup C\nsome college\nstandard\nnone\n76\n78\n75\n\n\n\n\n\n\n\n\n##value counts of educatioin level\nsample_data['parental level of education'].value_counts()\n\nsome college          226\nassociate's degree    222\nhigh school           196\nsome high school      179\nbachelor's degree     118\nmaster's degree        59\nName: parental level of education, dtype: int64\n\n\n\n#test preparation \nsample_data['test preparation course'].value_counts()\n\nnone         642\ncompleted    358\nName: test preparation course, dtype: int64"
  },
  {
    "objectID": "notebooks/Hypothesis-Testing.html#framework",
    "href": "notebooks/Hypothesis-Testing.html#framework",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "We start with a null hypothesis (H0) that represents the currect state ( Nothing going on, =, &lt;=, &gt;=)\nWe also have an alternative hypothesis(HA) that represents the research question we are testing. ( something is hoing on, !=, &gt;, &lt;)\nWe conduct a hypothesis test under the assumption that null hypothesis is true, either via simulation(Permutation test) or theoretical test using CLT.\n\nChoose a test statistic\nCompute the test statistic.\nDetermine the frequency distribution of the test statistic under the hypothesis.\nMake a decision using this distribution as a guide as discussed in the point-4.\n\nIf the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, we stick with the null hypothesis. If they do, then we reject the null hypothesis in the favor of the alternate hypothesis. If P value is low (Lower than significance level), we say that it is unlikely to observe the data if the null hypotheis were true, and hence reject the null hypothesis. If it is High, we won’t reject the null hypothesis.\n\nAlways construct hypotheses about population parameters (e.g. population mean, μ) and not the sample statistics (e.g. sample mean)\n\nNote that the alternative hypothesis might be one-sided (μ &lt; or &gt; the null value) or two-sided (μ≠ the null value), and the choice depends on the research question.\n\n\n\nImportant: P Value is the probability of obtaining a value of your test statistic that is at least as extream as what ws observed, under the assumption the null hypothesis is true. It is not the probability that the null hypothesis is True.\n\n\n\nNote: P Value = Conditional probability of data given null hypothesis is true = P(observed or more extreme sample statistic ∣ H0 true)\n\n\n\n\nAnalyze the problem and state the Null Hypothesis\nState the Alternate Hypothesis\nChoose a test statistic and Compute the test statistic\nDetermine the frequency distribution of the test statistic under the hypothesis.\nCalculate the P-Value for your CL based on two tail/single tail.\nMake a decision.\n\n\nYou can do one tailed or two tailed test based on the alternate hypothesis.\n\nA two sided Hypothesis with significance level alpha is equivalent to a confidence interval with \\(CL = 1 - \\alpha\\)\nA one sided Hypothesis with significance level alpha is equivalent to a confidence interval with \\(CL = 1- (2*\\alpha)\\)\n\n\n\n\n‘hypothesis cl’\n\n\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\n# figure size in inches\nrcParams['figure.figsize'] = 11.7,8.27\n\n\n#you can download the data from https://www.kaggle.com/spscientist/students-performance-in-exams\nsample_data = pd.read_csv(\"datasets_74977_169835_StudentsPerformance.csv\")\n\n\n##sample data \nsample_data.head()\n\n\n\n\n\n\n\n\ngender\nrace/ethnicity\nparental level of education\nlunch\ntest preparation course\nmath score\nreading score\nwriting score\n\n\n\n\n0\nfemale\ngroup B\nbachelor's degree\nstandard\nnone\n72\n72\n74\n\n\n1\nfemale\ngroup C\nsome college\nstandard\ncompleted\n69\n90\n88\n\n\n2\nfemale\ngroup B\nmaster's degree\nstandard\nnone\n90\n95\n93\n\n\n3\nmale\ngroup A\nassociate's degree\nfree/reduced\nnone\n47\n57\n44\n\n\n4\nmale\ngroup C\nsome college\nstandard\nnone\n76\n78\n75\n\n\n\n\n\n\n\n\n##value counts of educatioin level\nsample_data['parental level of education'].value_counts()\n\nsome college          226\nassociate's degree    222\nhigh school           196\nsome high school      179\nbachelor's degree     118\nmaster's degree        59\nName: parental level of education, dtype: int64\n\n\n\n#test preparation \nsample_data['test preparation course'].value_counts()\n\nnone         642\ncompleted    358\nName: test preparation course, dtype: int64"
  },
  {
    "objectID": "notebooks/Hypothesis-Testing.html#comparing-two-independent-samples",
    "href": "notebooks/Hypothesis-Testing.html#comparing-two-independent-samples",
    "title": "Hypothesis Testing",
    "section": "Comparing two independent samples",
    "text": "Comparing two independent samples\n\n2 groups must be independent with-in groups as well as between the groups.\nIf we have skewness in the sample distribution, we need more samples for hypothesis testing.\n\n\nProblem Statement\nWe have to check whether there is a difference in the math score in students who completed the preparation course and not.\n\n###math scores of test completed and not completed\nmath_score_with_test = sample_data['math score'][sample_data['test preparation course']=='completed']\nmath_score_wo_test = sample_data['math score'][sample_data['test preparation course']=='none']\n\n\n##no of students\nprint(\"No of students completed the course\", len(math_score_with_test))\nprint(\"No of students not completed the course\", len(math_score_wo_test))\n\nNo of students completed the course 358\nNo of students not completed the course 642\n\n\n\n#Plotting distribution of mathscores of compeleted students\nsns.distplot(math_score_with_test, hist=True, kde=True, color='red', norm_hist=True, label=\"Completed the course\")\nplt.legend()\n\n\n\n\n\n\n\n\n\n#Plotting distribution of mathscores of not compeleted students\nsns.distplot(math_score_wo_test, hist=True, kde=True, color='red', norm_hist=True, label=\"not completed the course\")\nplt.legend()\n\n\n\n\n\n\n\n\nStep-1: We can compare the mean scores of both groups. so Null Hypothesis is “There is no difference in both groups”\nH0 = There is no difference between the two groups.\nmu_group1 = mu_group2\nmu_group1 - mu_group2 = 0\n\nStep-2: Alternate Hypothesis is “There is a difference between the two groups”\nH0 = There is no difference between the two groups.\nmu_group1 != mu_group2\nmu_group1 - mu_group2 != 0\n\nStep-3: Test Statistic and Calculate the Observed Test Statistic\nT_obs = observed_group1_mean - observed_group2_mean = 5.617649106319291\n\nT_obs = math_score_with_test.mean() - math_score_wo_test.mean()\nT_obs\n\n5.617649106319291\n\n\nStep-4: Simulate the distribution using the permutation test\n#### Permutation simulation Let’s say we have n1 elements in group1 and n2 elements in the group2.\n\nCombine the group1, group2.\nfor each permutation data sample\n\ntake first n1 elements as group1, rest n2 samples as group2.\ncalculate the test statistic\n\n\n\n\nCode\n\n##we can get the permutations from np.random.permutations. \nfor i in range(5):\n    print(np.random.permutation([1, 2, 3, 4, 5]))\n\n[3 2 4 1 5]\n[4 3 5 1 2]\n[5 1 3 4 2]\n[4 2 3 1 5]\n[3 2 5 1 4]\n\n\n\ndata_sample = np.concatenate([math_score_with_test, math_score_wo_test])\nn1 = len(math_score_with_test)\ndist_test_stat = []\nnp.random.seed(85)\nfor i in range(1000000):\n    out = np.random.permutation(data_sample) ##random permutation\n    ts_cal = out[:n1].mean() - out[n1:].mean() ##getting stat difference\n    dist_test_stat.append(ts_cal)\n\n\nsns.distplot(dist_test_stat, hist=True, kde=True, color='red', norm_hist=True, label=\"Test Statistic distribution\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nprob = sum(np.array(dist_test_stat)&gt;=T_obs)/1000000\n\n\nprob\n\n1e-06\n\n\nStep-5: Calculation the P-Value\nFor two tailed test \\(\\text{P Value} = 2*min([0.5, prob, 1-prob])\\)\none tailed test with ‘greater than’ alternate hypothesis \\(\\text{P Value} = prob\\)\none tailed test with ‘less than’ alternate hypothesis \\(\\text{P Value} = 1- (prob)\\)\n\nP_value = 2*np.min([0.5, prob, 1-prob])\nprint('P_value', P_value)\n\nP_value 2e-06\n\n\nStep-6: Make a Decision:\nFor Significance Level of 5% (95% CL), P_value is very less so we reject the null hypothisis in favour of alternate.\n\nNote: In place of Test Statistic, you can use any formulation even t test stat also.\n\nYou can do all the avove with permute module of Python.\n\n\nCode with permute module\n\nfrom permute.core import two_sample\nsample_test = two_sample(x=math_score_with_test, y=math_score_wo_test, reps=100000, \n                         stat='mean', alternative='two-sided', keep_dist=True, seed=10)\n\n\nsns.distplot(sample_test[2], hist=True, kde=True, color='red', norm_hist=True, label=\"Test Statistic distribution\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nprint(\"observed test Statistic\", sample_test[1])\nprint(\"P-Value\", sample_test[0])\n\nobserved test Statistic 5.617649106319291\nP-Value 1.999980000199998e-05\n\n\n\n\nAnother way to test\n\nGet the CI of the mean(any stat) of two samples.\nIf those CI not overlap, there is a difference between those.\n\n\nfrom arch.bootstrap import IIDBootstrap\nbs = IIDBootstrap(math_score_with_test)\nprint('Studentized-t CI of scores of test completed students--&gt;', bs.conf_int(func=np.mean, reps=1000, method='bca', size=0.95))\n\nStudentized-t CI of scores of test completed students--&gt; [[68.34468215]\n [71.1424581 ]]\n\n\n\nbs = IIDBootstrap(math_score_wo_test)\nprint('Studentized-t CI of scores of test not completed students--&gt;', bs.conf_int(func=np.mean, reps=1000, method='bca', size=0.95))\n\nStudentized-t CI of scores of test not completed students--&gt; [[62.91883691]\n [65.28373942]]\n\n\nThere is no overlap between these 95% CI, so there is a difference.\n\nTip: What if we want to check mu_group1 - mu_group2 = some_number? We can formulate that as mu_group1 - mu_group2 - some_number = 0 =&gt; mu_group1 - (mu_group2 + some_number) = 0 or (mu_group1 - some_number) - mu_group2 = 0"
  },
  {
    "objectID": "notebooks/Hypothesis-Testing.html#comparing-two-dependent-samplespaired-test",
    "href": "notebooks/Hypothesis-Testing.html#comparing-two-dependent-samplespaired-test",
    "title": "Hypothesis Testing",
    "section": "Comparing two dependent samples(Paired Test):",
    "text": "Comparing two dependent samples(Paired Test):\nThis type of test may be needed for pre-post study on the same people or repeated measures on the same set of people. In our sample data, let’s say we want to compare scores of reading and writing that are equal or not.\n\nIn the above problem, students are same in the both samples. so there is a dependency, they are not independent.\nTwo groups must be of same size because its a paired data.(n1=n2=n)\n\n\nHow to solve?\n\nGet the difference(any value that you want to find) of two samples, so we will get n samples.\nDo 1 sample hypothesis test to find the difference is zero or not. or use bootstrapping to get the CI and check.\n\n\n\nOne-sample Hypothesis test using permutation\nWe have to check whether our mean(any stat) is centered around the zero or not. We don’t have two samples to permute so we can add random positive and negative signs and we can shuffle those signs to get the hypothesis null distribution.\n\nreading_score = sample_data['reading score']\nwriting_score = sample_data['writing score']\ndiff_score = reading_score - writing_score\n\nStep-1: Null Hypothesis is “difference of both scores is zero”\nH0 = the difference between both scores is zero.\nmean_diff_score = 0\n\nStep-2: Alternate Hypothesis is “There is a difference in both scores”\nH0 = There is a difference in both scores.\nmean_diff_score != 0\n\nStep-3: Test Statistic and Calculate the Observed Test Statistic\nT_obs = mean(diff_score)\n\nT_obs = diff_score.mean()\nprint(T_obs)\n\n1.115\n\n\nStep-4: Simulate the distribution using the permutation test\n\n\none sample Permutation simulation:\nLet’s say we have n elements.\n\nfor M number of iterations\n\nGenerate random negative and positive signs of length n. \nassign those signs to the elements\ncalculate the statistic value.\n\n\n\n\nCode\n\ndef generate_n_random_signs(n):\n    return 1- 2* np.random.binomial(1, 0.5, size=n)\ngenerate_n_random_signs(10)*np.random.randint(1, 10, 10)\n\narray([ 9,  9,  4,  4, -4, -4,  9,  3, -3,  1])\n\n\n\ndist_test_stat = []\nnp.random.seed(85)\nn = len(diff_score)\nfor i in tqdm(range(1000000)):\n    out = generate_n_random_signs(n) * diff_score\n    ts_cal = out.mean()\n    dist_test_stat.append(ts_cal)\n\n100%|██████████████████████████████████████████████████████████████████████| 1000000/1000000 [08:04&lt;00:00, 2062.25it/s]\n\n\n\n##plotting\nsns.distplot(dist_test_stat, hist=True, kde=True, color='red', norm_hist=True, label=\"Test Statistic distribution\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nprob = sum(np.array(dist_test_stat)&gt;=T_obs)/1000000\n\nStep-5: Calculation the P Value\n\nP_value = 2*np.min([0.5, prob, 1-prob])\nprint('P_value', P_value)\n\nP_value 2e-06\n\n\nStep-6: Make a Decision:\nFor Significance Level of 5% (95% CL), P value is very less so we reject the null hypothisis in favour of alternate.\nYou can do all the avove with permute module of Python.\n\n\nCode with permute module\n\nfrom permute.core import one_sample\none_sample_test = one_sample(x=diff_score, reps=100000, \n                         stat='mean', alternative='two-sided', keep_dist=False, seed=10)\n\n\nprint(\"observed test Statistic\", one_sample_test[1])\nprint(\"P-Value\", one_sample_test[0])\n\n(1.999980000199998e-05, 1.115)"
  },
  {
    "objectID": "notebooks/Hypothesis-Testing.html#comparing-n-independent-samples-permutation-anova",
    "href": "notebooks/Hypothesis-Testing.html#comparing-n-independent-samples-permutation-anova",
    "title": "Hypothesis Testing",
    "section": "Comparing N independent samples: ( Permutation ANOVA)",
    "text": "Comparing N independent samples: ( Permutation ANOVA)\n\nN groups must be independent with-in groups as well as between the groups.\nIf we have skewness in the sample distribution, we need more samples for hypothesis testing.\n\nYou can read about ANOVA at this or this.\n\n\nOne-Way ANOVA:\nUsing one way ANOVA, we can compare the K groups variability. Those K groups must be independent.\nTest statistic for one way ANOVA = \\(\\sum_{k=1}^K n_k(\\overline{X_k} - \\overline{X})^2\\)\n\\(n_k\\) = number of samples in group k\n\\(\\overline{X_k}\\) = Mean of group k\n\\(\\overline{X}\\) = Total mean (All groups)\nExcept for the test statistic, everything is the same, we will permute the groups and calculate the test stat distribution.\n\n\nSteps:\n\nNull Hypothesis - all the means are equal\nAlternate Hypothesis - all means are not equal\nCalculate the observed test statistic\nn times:\n4.1. permute the values. take the first n1 as the first group, next n2 as the second group, .. nk as kth group.\n4.2. calculate the test statistic using the above formula and add to the final dist.\n\nPlot the distribution and calculate the P-value.\n\n\nYou can do above all with permute\n\nsample_data['parental level of education'].value_counts()\n\nsome college          226\nassociate's degree    222\nhigh school           196\nsome high school      179\nbachelor's degree     118\nmaster's degree        59\nName: parental level of education, dtype: int64\n\n\n\nmath_scores = sample_data['math score']\neducation_groups = sample_data['parental level of education'].values\n\nNull Hypothesis = math scores of every group of education is same.\nAlternate Hypothesis = math scores are different in groups.\n\nfrom permute.ksample import k_sample\nns_oneway_anova = k_sample(x=math_scores, group=education_groups, reps=100000, stat='one-way anova', keep_dist=True, seed=10 )\n\n\nsns.distplot(ns_oneway_anova[2], hist=True, kde=True, color='red', norm_hist=True, label=\"Test Statistic distribution\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nprint(\"observed test Statistic\", ns_oneway_anova[1])\nprint(\"P-Value\", ns_oneway_anova[0])\n\nobserved test Statistic 7295.561831098111\nP-Value 9.99990000099999e-06\n\n\nWe can do two way anowas with permute.ksample.bivariate_k_sample"
  },
  {
    "objectID": "notebooks/Spectrum Full Fine Tuning.html",
    "href": "notebooks/Spectrum Full Fine Tuning.html",
    "title": "Spectrum Full Fine Tuning",
    "section": "",
    "text": "Spectrum is a new method for full fine-tuning pre-trained language models. It is an approach for efficient LLM fine-tuning that selectively tunes only a small subset of layers based on their SNR (Signal to Noise Ratio).\nIt Uses Random Matrix Theory (RMT) especially Marchenko-Pastur Law to predict the SNR of each layer. This law charecterizes eigan value distributions in large random matrices.\nTo understand this better, let’s first understand the basics of Linear Algebra.\n\nSingular Value Decomposition (SVD)\n\nSingular Value Decomposition (SVD) is a matrix factorization technique that decomposes a matrix into three other matrices. It is represented as:\n\\[\\begin{align}\nW = U \\Sigma V^T\n\\end{align}\\]\n\n\\(W\\) is the input matrix\n\\(U\\) is an orthogonal matrix\n\\(\\Sigma\\) is a diagonal matrix containing the singular values (\\(\\sigma_i\\)) of \\(W\\)\n\\(V^T\\) is the transpose of an orthogonal matrix \\(V\\)\n\nWhen applied to the weight matrix of neural networks,\n\nIt helps us identify the most important patterns in the weight matrix by looking at the largest singular values (dominant patterns/directions in weight matrix). Least important patterns (noise) are represented by smaller singular values.\nIt can help us understand the underlying structure of the model’s weights and potentially identify which parts of the model are most important.\nRemoving the smallest singular values (noise) might suppress factual knowledge or impact the learning of more complex patterns, which can lead to catastrophic forgetting.\n\n\nEigan Value Decomposition (EVD)\n\nEigan Value Decomposition (EVD) is a matrix factorization technique that decomposes a matrix into two matrices. It is similar to SVD but for square matrices. It is represented as:\n\\[\\begin{align}\nW = Q \\Lambda Q^T\n\\end{align}\\]\n\n\\(W\\) is the input matrix\n\\(Q\\) is an orthogonal matrix\n\\(\\Lambda\\) is a diagonal matrix containing the eigan values (\\(\\lambda_i\\)) of \\(W\\)\n\n\nRelation between SVD and EVD\n\nSVD of a matrix \\(W\\) of shape \\((m \\times n)\\) is given as \\(W = U \\Sigma V^T\\)\nlets look at \\(W.W^T\\)\n\\[\\begin{align}\nW.W^T = (U \\Sigma V^T)(U \\Sigma V^T)^T = U \\Sigma^2 U^T\n\\end{align}\\]\n\\(W.W^T\\) is a square matrix and can be decomposed using EVD as \\(W.W^T = Q \\Lambda Q^T\\)\nso \\(\\Lambda_{W.W^T} = \\Sigma^2_{W}\\)\n\nMarchenko-Pastur Law\n\nIn random matrix theory, the Marchenko-Pastur Law describes the asymptotic behavior of the eigenvalues of large random matrices. It tells that, For a matrix \\(W\\) of shape \\((m \\times n)\\) with \\(m \\geq n\\), the eigenvalues of \\(\\frac{1}{n}W.W^T\\) converge to a distribution bounded by:\n\\[\\begin{align}\n\\lambda \\in \\left[\\sigma^2 \\left(1 - \\sqrt{\\frac{m}{n}}\\right)^2, \\sigma^2 \\left(1 + \\sqrt{\\frac{m}{n}}\\right)^2\\right]\n\\end{align}\\]\nthe bound for singular values of \\(W\\) is given by:\n\\[\\begin{align}\n\\sigma \\in \\left[ \\frac{1}{\\sqrt{n}} \\sigma \\left(1 - \\sqrt{\\frac{m}{n}}\\right) , \\frac{1}{\\sqrt{n}} \\sigma \\left(1 + \\sqrt{\\frac{m}{n}}\\right)\\right]\n\\end{align}\\]\n\nSignal to Noise Ratio (SNR)\n\nfrom above, maximum threshold for the singular value random matrix is \\(\\sigma_{\\text{thresh}} = \\sigma \\left(1 - \\sqrt{\\frac{m}{n}}\\right)\\)\n\nRemoving normalization factor \\(\\frac{1}{\\sqrt{n}}\\) to ensure numerical stability.\nCalculating \\(\\sigma\\) using IQR instead of std to account for potential skewness and kurtosis.\n\nlets take \\(S\\) contains the singular values of \\(W\\), then SNR is given by:\n\\[\\begin{align}\nSNR = \\frac{\\text{count of singular values in S} \\gt \\sigma_{thresh}}{\\text{count of singular values in S} \\leq \\sigma_{thresh}}\n\\end{align}\\]\n\nnormalizing SNR by the largest singular value $ $ for sensitivity analysis, enhanced comparison\nMatrices with higher SNR contain more informative features and less noise.\n\n\nLayer Selection\n\n\n\nFor each layer, calculate SVD of weight matrix \\(W\\) and then calculate SNR and normalize it.\nselect top \\(k\\%\\) layers with highest SNR. K is a hyperparameter.\n\nReference: 1. https://arxiv.org/abs/2406.06623 2. https://huggingface.co/blog/anakin87/spectrum"
  },
  {
    "objectID": "notebooks/KNN-Summary.html",
    "href": "notebooks/KNN-Summary.html",
    "title": "Summary - KNN Algorithm",
    "section": "",
    "text": "\\(N\\) - Number of data points\n\\(X_q\\) - query data points\n\\(X_{qn}\\) - nth query data point\n\\(X\\) - input train data\n\\(D\\) - dimensionality of data\n\\(C\\) - Number of classes\n\\(C_i\\) - i^th class\n\\(N_k\\) - K nearst neighbors\n\\(m\\) - Number of epochs in SGD\nThis blog was originally published in this link."
  },
  {
    "objectID": "notebooks/KNN-Summary.html#terminology",
    "href": "notebooks/KNN-Summary.html#terminology",
    "title": "Summary - KNN Algorithm",
    "section": "",
    "text": "\\(N\\) - Number of data points\n\\(X_q\\) - query data points\n\\(X_{qn}\\) - nth query data point\n\\(X\\) - input train data\n\\(D\\) - dimensionality of data\n\\(C\\) - Number of classes\n\\(C_i\\) - i^th class\n\\(N_k\\) - K nearst neighbors\n\\(m\\) - Number of epochs in SGD\nThis blog was originally published in this link."
  },
  {
    "objectID": "notebooks/KNN-Summary.html#algorithm",
    "href": "notebooks/KNN-Summary.html#algorithm",
    "title": "Summary - KNN Algorithm",
    "section": "Algorithm",
    "text": "Algorithm\n\nGiven a query point \\(X_{qn}\\), find the \\(k\\) nearest points in \\(X\\). (You can use any distance metric)\nCount which class has maximum points in those \\(k\\) nearest points and classify given query point \\(X_{qn}\\) belongs to the same class. We can give weightage to the nearest points based on the distance (any function of distance).\nProbability of belonging to the specific class in a classification scenario is\n\n\n\\[\\begin{align}\nP(Y=C_i|X_q) = \\frac{1}{K}\\sum_{i\\epsilon N_k}I(y_i==C_i) \\\\I(true) = 1, I(false) = 0\n\\end{align}\\]\n\n\nIf we use KNN to solve the regression problem, we can get the average of nearest points and gives as output.\n\n\n\\[\\begin{align}\nY_{X_q} = \\frac{1}{K}\\sum_{i\\epsilon N_k}y_i\n\\end{align}\\]"
  },
  {
    "objectID": "notebooks/KNN-Summary.html#useful-points-to-know",
    "href": "notebooks/KNN-Summary.html#useful-points-to-know",
    "title": "Summary - KNN Algorithm",
    "section": "Useful points to know",
    "text": "Useful points to know\n\nThere is no need for any specific process to work with multi-class classification because it works based on nearest neighbors.\nThere is no need for training if we want to use brute force search to get k-nearest neighbors. If we want to use Tree-based/LSH based/graph-based searching, we have to create a corresponding Tree/LSH forest/graph in the training time.\nTesting time complexity is \\(O(ND)\\) if we use brute force search, it is very huge if we have more data points to train. If we need less time complexity go for Tree/LSH/graph-based neighbor search. Many tree algorithms give complexity of \\(O(D*log(N))\\).\n\n\nImportant: You can check the benchmarking of NN searching algorithms in this paper and this GitHub\n\n\nIt works based on distance measure so scaling the features is very important.\nChoosing a distance metric is very crucial. If we require a rotation-invariant distance metric then Euclidean distance is one of the best choices. But in high dimensions, a curious phenomenon arises: the ratio between the nearest and farthest points approaches 1, i.e. the points essentially become uniformly distant from each other. This phenomenon can be observed for a wide variety of distance metrics, but it is more pronounced for the Euclidean metric than, say, the Manhattan distance metric. You can read more about this in this paper.\n\n\nTip: High-dimensional spaces tend to be extremely sparse, which means that every point is far away from virtually every other point, and hence pairwise distances tend to be uninformative so before applying nearest-neighbor classification it is a good idea to plot a histogram of pairwise distances of a data to see if they are sufficiently varied.\n\n\nHigh-dimensional spaces may not give better results so using one-hot vectors may not be useful if have more categorical variables. so, try with mean encoding/frequency-based encoding or use dimensionality reduction techniques/feature selection to get the lower dimensionality data.\nThis is not a linear model, so you can classify the non-linear data.\nAn increase in the feature value not always leads to an increase or decrease in the target outcome/probability so, it is not a monotone model.\nIt won’t consider the interaction between the features. We have to create the interaction features if we need it. More interaction features may lead to less interpretability."
  },
  {
    "objectID": "notebooks/KNN-Summary.html#hyperparameters",
    "href": "notebooks/KNN-Summary.html#hyperparameters",
    "title": "Summary - KNN Algorithm",
    "section": "Hyperparameters",
    "text": "Hyperparameters\n\nK is the Hyperparameter in K-NN.\nThe low value of K will give more variance to the model. increasing the K, reduces the variance, and increases the bias of the model.\nYou can check the decision surface of KNN for different K values in the below image."
  },
  {
    "objectID": "notebooks/KNN-Summary.html#interpretability",
    "href": "notebooks/KNN-Summary.html#interpretability",
    "title": "Summary - KNN Algorithm",
    "section": "Interpretability",
    "text": "Interpretability\n\nThere is a lack of global model interpretability because the model is inherently local and there are no global weights or structures explicitly learned.\nWe can get interpretability based on the data we have i.e. datapoint \\(x_1\\) is similar to the datapoint \\(x_2\\) and \\(x_1\\) caused \\(y\\) so we predict that \\(x_2\\) will cause \\(y\\) as well. So, we can get the k nearest neighbors and we can analyze/interpret those data points.\nIf we have thousands of features, it is very difficult to analyze and get interpretability."
  },
  {
    "objectID": "notebooks/KNN-Summary.html#references",
    "href": "notebooks/KNN-Summary.html#references",
    "title": "Summary - KNN Algorithm",
    "section": "References",
    "text": "References\n\nhttps://stackexchange.com/\nhttps://arxiv.org/pdf/1807.05614.pdf\nhttps://bib.dbvis.de/uploadedFiles/155.pdf\nhttps://helloacm.com/wp-content/uploads/2016/03/2012-10-26-knn-concept.png"
  },
  {
    "objectID": "notebooks/adapter_tuning_PEFT.html",
    "href": "notebooks/adapter_tuning_PEFT.html",
    "title": "PEFT - Adapter Tuning",
    "section": "",
    "text": "Large pre-trained language models (e.g., BERT, GPT) have revolutionized NLP tasks by leveraging massive amounts of unlabeled data. Transfer learning involves first pre-training these models on large corpora and then fine-tuning them on smaller, task-specific datasets. However, fine-tuning all the parameters of a model like BERT is computationally expensive and inefficient, particularly when there are multiple downstream tasks\n\nAdapter Layers\n\n\nAdapters are small, task-specific layers added between the layers of the pre-trained model.\nInstead of fine-tuning all the parameters of the model, only the parameters of the adapter layers are updated during training for a specific task. The rest of the model’s parameters remain frozen.\nThis method significantly reduces the number of trainable parameters and, thus, the computational cost of fine-tuning.\n\n\nBasic Adapter Design\n\n\n\nEach adapter consists of a down-projection, a non-linearity, and an up-projection as shown in above image\nThe down-projection reduces the dimensionality of the intermediate layer activations, and the up-projection restores it, thus keeping the adapter small and efficient.\nThe adapters first project the original d-dimensional features into a smaller dimension, m, apply a nonlinearity, then project back to d dimensions.\nso The total number of parameters added per layer, including biases, is 2md + d + m.\nBy setting m &lt;&lt; d, we limit the number of parameters added per task.\n\n\nAdapter Fusion\n\n\nSequential fine-tuning and multi-task learning are methods aiming to incorporate knowledge from multiple tasks; however, they suffer from catastrophic forgetting and difficulties in dataset balancing.\nAdapterFusion addresses this by non-destructively composing multiple tasks. Rather than overwriting model parameters for each task, the method fuses information from different adapters to solve a new task.\nalgorithm:\n\ntrain a adapter layers for each task seperatly.\nAdapterFusion learns a weighted combination of previously trained all adapters as shown in below figure.\n\n\n\n\nThis fusion mechanism allows the model to leverage knowledge from all tasks in a modular fashion.\nThe adapters themselves remain independent, and the fusion weights can be tuned to emphasize adapters that are most relevant for a specific task.\n\n\nCOMPACTER (Compact Adapter)\n\nCOMPACTER is combination of Hypercomplex Adapter Layers using Kronecker Product, Low-Rank Approximation, shared weights across adapters.\n\nKronecker Product:\n\n\n\nHypercomplex Adapter Layers:\n\nIn the adapter layers, previously it used the FC layers as below\n\\[\\begin{align}\ny = Wx + b \\quad \\text{where } W \\text{ is } (m \\times d)\\\\\n\\end{align}\\]\nW will be replaced using Kronecker Product of two matrices like below\n\\[\\begin{align}\nW = \\sum_{i=1}^n A_i \\otimes B_i \\\\\nA_i \\text{ is } (n \\times n) \\quad , \\quad B_i \\text{ is } (\\frac{m}{n} \\times \\frac{d}{n}) \\\\\n\\end{align}\\]\nn is user defined hyper-parameter. d, m are must divisible by n\nBelow is the illustration of Hypercomplex Adapter Layers. It is sum of Kronecker Product of matrices \\(A_i\\), \\(B_i\\) and here n = 2, d = 8, m = 6\n\nNo of parameters to tune here is reduced compared to FC layer as shown above.\nThis layer is generalization of the FC layer via the hyperparameter n. \n\nwhen n = 1, \\(W = A_1 \\otimes B_1 = aB_1\\) (a is the single element of the 1×1 matrix), B1 matrix is of shape \\(m \\times d\\)\nSince learning a and \\(B_1\\) separately is equivalent to learning their multiplication jointly, scalar a can be dropped, which is learning the single weight matrix in an FC layer\n\n\nLow Rank Parameterization and Sharing information across adapters\n\n\n\\(A_i\\) are shared parameters that are common across all adapter layers while \\(B_i\\) are adapter-specific parameters.\nassumption is the model can also be effectively adapted by learning transformations in a low-rank subspace and \\(B_i\\) is divided into multiplication of two low rank matrices\n\n\\[\\begin{align}\nW = \\sum_{i=1}^n A_i \\otimes B_i = \\sum_{i=1}^n A_i \\otimes (s_i t_i)\\\\\ns_i \\text{ is } (\\frac{m}{n} \\times r) \\quad t_i \\text{ is } (r \\times \\frac{d}{n}) \\quad \\text{matrix}\n\\end{align}\\]\nReference:\n\nhttps://arxiv.org/pdf/1902.00751\nhttps://arxiv.org/pdf/2005.00247\nhttps://arxiv.org/pdf/2102.08597"
  },
  {
    "objectID": "notebooks/Advanced-Feature-Extraction.html",
    "href": "notebooks/Advanced-Feature-Extraction.html",
    "title": "Advanced Feature Extraction from Text",
    "section": "",
    "text": "In the previous article, I discussed basic feature extraction methods like BOW, TFIDF but, these are very sparse in nature. In this tutorial, we will try to explore word vectors this gives a dense vector for each word. There are many ways to get the dense vector representation for the words. below are some of them"
  },
  {
    "objectID": "notebooks/Advanced-Feature-Extraction.html#co-occurrence-matrix-and-svd",
    "href": "notebooks/Advanced-Feature-Extraction.html#co-occurrence-matrix-and-svd",
    "title": "Advanced Feature Extraction from Text",
    "section": "Co-occurrence Matrix and SVD",
    "text": "Co-occurrence Matrix and SVD\nWe can create a co-occurrence matrix of text and then get a low rank approximation of matrix to get the dense feature representation.\nTo create a co-occurrence matrix, you go through text setting a window size around each word. You then keep track of which words appear in that window.\nlets create co-occurrence matrix with below sentences.\n\nsent_list = ['I like deeplearning.', 'I like NLP.', 'NLP is awesome.']\n\n\nwith window size of 1. the co-occurrence matrix is\n\n\n\n\ncoomatrix\n\n\nlike word came in context of i 2 times in window size one. in similar way, I updated above co-occurrence matrix with all counts.\n\n\nCode\nI have written a brute force version of code below.\n\nimport tensorflow as tf\nimport numpy as np\ndef cooccurrence_matrix(distance,sentances):\n    '''\n    Returns co-occurrence matrix of words with in a distance of occurrrence\n    input:\n    distance: distance between words(Window Size)\n    sentances: documets to check ( a list )\n    output:\n    co-occurance matrix in te order of list_words order\n    words list\n    '''\n    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n    tokenizer.fit_on_texts(sentances)\n    list_words = list(tokenizer.word_index.keys())\n    #print(list_words)\n    #length of matrix needed\n    l = len(list_words)\n    #creating a zero matrix\n    com = np.zeros((l,l))\n    #creating word and index dict\n    dict_idx = {v:i for i,v in enumerate(list_words)}\n    for sentence in sentances:\n        sentence = tokenizer.texts_to_sequences([sentence])[0]\n        tokens = [tokenizer.index_word[i] for i in sentence]\n        #tokens= sentence.split()\n        for pos,token in enumerate(tokens):\n            #if eord is in required words\n            if token in list_words:\n                #start index to check any other word occure or not\n                start=max(0,pos-distance)\n                #end index\n                end=min(len(tokens),pos+distance+1)\n                for pos2 in range(start,end):\n                    #if same position\n                    if pos2==pos:\n                        continue\n                    # if same word\n                    if token == tokens[pos2]:\n                        continue\n                    #if word found is in required words\n                    if tokens[pos2] in list_words:\n                        #index of word parent\n                        row = dict_idx[token]\n                        #index of occurance word\n                        col = dict_idx[tokens[pos2]]\n                        #adding value to that index\n                        com[row,col] = com[row,col] + 1\n    return com, list_words\n\n\ncoo = cooccurrence_matrix(1, sent_list)\nprint(coo[1])\nprint(coo[0])\n\n['i', 'like', 'nlp', 'deeplearning', 'is', 'awesome']\n[[0. 2. 0. 0. 0. 0.]\n [2. 0. 1. 1. 0. 0.]\n [0. 1. 0. 0. 1. 0.]\n [0. 1. 0. 0. 0. 0.]\n [0. 0. 1. 0. 0. 1.]\n [0. 0. 0. 0. 1. 0.]]\n\n\nNow we can use SVD to get low rank approximation matrix(This will give dense matrix)\n\nfrom sklearn.decomposition import TruncatedSVD\ntsvd = TruncatedSVD(n_components=3, n_iter=10, random_state=32 )\ndense_vector = tsvd.fit_transform(coo[0], )\ndense_vector\n\narray([[ 1.94649798e+00,  2.73880515e-15, -2.49727487e-01],\n       [-2.40633313e-15,  2.43040910e+00, -2.56144970e-01],\n       [ 1.20300191e+00,  1.58665133e-15,  4.04067562e-01],\n       [ 9.73248989e-01,  1.21830556e-15, -1.24863743e-01],\n       [ 7.73781546e-16,  5.73741760e-01,  1.08504750e+00],\n       [ 2.29752921e-01,  3.09635046e-16,  5.28931305e-01]])\n\n\n\nprint(\"Vector of \", \"'\" , coo[1][1], \"'\", \"is \", dense_vector[1])\n\nVector of  ' like ' is  [-2.40633313e-15  2.43040910e+00 -2.56144970e-01]"
  },
  {
    "objectID": "notebooks/Advanced-Feature-Extraction.html#word2vec",
    "href": "notebooks/Advanced-Feature-Extraction.html#word2vec",
    "title": "Advanced Feature Extraction from Text",
    "section": "Word2Vec",
    "text": "Word2Vec\nI think, there are many articles and videos regarding the Mathematics and Theory of Word2Vec. So, I am giving some links to explore and I will try to explain code to train the custom Word2Vec. Please check the resources below.\n\nyoutube: https://www.youtube.com/watch?list=PLUOY9Q6mTP21Al_odE-v_lmHDjVMSO9BX&v=SSpSk1Io52w&feature=emb_title\n\n\nYou can read a good blog here\n\nPlease watch the above videos or read the above blog before going into the coding part.\n\nWord2Vec using Gensim\nWe can train word2vec using gensim module with CBOW or Skip-Gram ( Hierarchical Softmax/Negative Sampling). It is one of the efficient ways to train word vectors. I am training word vectors using gensim, using IMDB reviews as a data corpus to train. In this, I am not training the best word vectors, only training for 10 iterations.\n\nTo train gensim word2vec module, we can give a list of sentences or a file a corpus file in LineSentence format. Here I am creating a list of sentences from my corpus. If you have huge data, please try to use LineSentence format to efficiently train your word vectors.\n\n##getting sentence wise data\nlist_sents = [nltk.word_tokenize(sent) for sent_tok in data_imdb.review for sent in nltk.sent_tokenize(sent_tok)]\n\n Training gensim word2vec as below\n\n##import gensim\nfrom gensim.models import Word2Vec\n##word2vec model ##this may take some time to execute. \nword2vec_model = Word2Vec(list_sents,##list of sentences, if you don;t have all the data in RAM, you can give file name to corpus_file \n                          size=50, ##output size of word emebedding \n                          window=4, ##window size\n                          min_count=1, ## ignors all the words with total frquency lower than this\n                          workers=5, ##number of workers to use\n                          sg=1, ## skip gram\n                          hs=0, ## 1 --&gt; hierarchical, 0 --&gt; Negative sampling\n                          negative=5, ##How many negative samples\n                          alpha=0.03, ##The initial learning rate\n                          min_alpha=0.0001, ##Learning rate will linearly drop to min_alpha as training progresses.\n                          seed = 54, ##random seed\n                          iter=10,\n                         compute_loss=True)##number of iterations\n\nYou can get word vectors as below\n\n##getting a word vector\nword2vec_model.wv['movie']\n\nYou can get most similar positive words for any given word as below\n\n##getting most similar positive words\nword2vec_model.wv.most_similar(positive='movie')\n\nYou can save your model as below\n\n##saving the model\nword2vec_model.save('w2vmodel/w2vmodel')\n\nYou can get the total notebook in the below GitHub link\n\ngithub: https://github.com/UdiBhaskar/Natural-Language-Processing/blob/master/Feature%20Extraction%20Methods/Advanced%20feature%20extraction%20-%20W2V/W2V_using_Gensim.ipynb\n\n\n\nWord2Vec using Tensorflow ( Skip-Gram, Negative Sampling)\nIn the negative sampling, we will get a positive pair of skip-grams and for every positive pair, we will generate n number of negative pairs. I used only 10 negative pairs. In the paper, they suggesting around 25. Now we will use these positive and negative pairs and try to create a classifier that differentiates both positive and negative samples. While doing this, we will learn the word vectors. We have to train a classifier that differentiates positive sample and negative samples, while doing this we will learn the word embedding. Classifier looks like below image\n\n\n\n\nkerasNS\n\n\n\nThe above model takes two inputs center word, context word and, model output is one if those two words occur within a window size else zero.\n\nPreparing the data\nWe have to generate the skip-gram pairs and negative samples. We can do that easily using tf.keras.preprocessing.sequence.skipgrams. This also takes a probability table(sampling table), in which we can give the probability of that word to utilize in the negative samples i.e. we can make probability low for the most frequent words and high probability for the least frequent words while generating negative samples.\nConverted total words into the number sequence. Numbers are given in descending order of frequency.\n\n##to use tf.keras.preprocessing.sequence.skipgrams, we have to encode our sentence to numbers. so used Tokenizer class\ntokenizer = tf.keras.preprocessing.text.Tokenizer()\ntokenizer.fit_on_texts(list_sents)\nseq_texts = tokenizer.texts_to_sequences(list_sents) ##list of list+\n\nIf we create total samples at once, it may take so much RAM and that gives the resource exhaust error. so created a generator function which generates the values batchwise.\n\n##Skipgram with Negativive sampling generator \n##for generating the skip gram negative samples we can use tf.keras.preprocessing.sequence.skipgrams and \n#internally uses sampling table so we need to generate sampling table with tf.keras.preprocessing.sequence.make_sampling_table\nsampling_table_ns = tf.keras.preprocessing.sequence.make_sampling_table(size=len(tokenizer.word_index)+1,   \n                                                                        sampling_factor=1e-05)\ndef generate_sgns():\n    ##loop through all the sequences\n    for seq in seq_texts:\n        generated_samples, labels = tf.keras.preprocessing.sequence.skipgrams(sequence=seq, \n                                                                      vocabulary_size=len(tokenizer.word_index)+1, \n                                                                      window_size=3, negative_samples=10, \n                                                                      sampling_table=sampling_table_ns)\n        length_samples = len(generated_samples)\n        for i in range(length_samples):\n            ##centerword, context word, label\n            yield [generated_samples[i][0]], [generated_samples[i][1]], [labels[i]]\n\n##creating the tf dataset\ntfdataset_gen = tf.data.Dataset.from_generator(generate_sgns, output_types=(tf.int64, tf.int64, tf.int64))\ntfdataset_gen = tfdataset_gen.repeat().batch(2048).prefetch(tf.data.experimental.AUTOTUNE)\n\n\n\nCreating Model\n\n##fixing numpy RS\nnp.random.seed(42)\n\n##fixing tensorflow RS\ntf.random.set_seed(32)\n\n##python RS\nrn.seed(12)\n\ntf.keras.backend.clear_session()\n\n##model\ndef getSGNS():\n    \n    center_word_input= Input(shape=(1,), name=\"center_word_input\")\n    context_word_input= Input(shape=(1,), name=\"context_word_input\")\n    \n    ##i am initilizing randomly. But you can use predefined embeddings. \n    embedd_layer = Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=100,\n                    embeddings_initializer=tf.keras.initializers.RandomUniform(seed=45),\n                     name=\"Embedding_layer\")\n    \n    #center word embedding\n    center_wv = embedd_layer(center_word_input)\n    \n    #context word embedding\n    context_wv = embedd_layer(context_word_input)\n    \n    #dot product\n    dot_out = Dot(axes=2, name=\"dot_between_center_context\")([center_wv, context_wv]) \n    \n    dot_out = Reshape((1,), name=\"reshaping\")(dot_out)\n    \n    final_out = Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.glorot_uniform(seed=54),\n                  name=\"output_layer\")(dot_out)\n    \n    basic_w2v = Model(inputs=[center_word_input, context_word_input], outputs=final_out, name=\"sgns_w2v\")\n    \n    return basic_w2v\n\n\nsgns_w2v = getSGNS()\n\n\n\nTraining\n\n##training\n\n##optimizer\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n\n##train step function to train\n@tf.function\ndef train_step(input_center, input_context, output_vector, loss_fn):\n    with tf.GradientTape() as tape:\n        #forward propagation\n        output_predicted = sgns_w2v(inputs=[input_center, input_context], training=True)\n        #loss\n        loss = loss_fn(output_vector, output_predicted)\n    #getting gradients\n    gradients = tape.gradient(loss, sgns_w2v.trainable_variables)\n    #applying gradients\n    optimizer.apply_gradients(zip(gradients, sgns_w2v.trainable_variables))\n    return loss, gradients\n\n##number of epochs\nno_iterations=100000\n\n##metrics # Even if you use .fit method, it alsocalculates batchwise loss/metric and aggregates those.  \ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\n\n#tensorboard file writers\nwtrain = tf.summary.create_file_writer(logdir='/content/drive/My Drive/word2vec/logs/w2vns/train')\n\n##creating a loss object for this classification problem\nloss_function = tf.keras.losses.BinaryCrossentropy(from_logits=False, \n                                                                reduction='auto')\n\n##check point to save\ncheckpoint_path = \"/content/drive/My Drive/word2vec/checkpoints/w2vNS/train\"\nckpt = tf.train.Checkpoint(optimizer=optimizer, model=sgns_w2v)\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n\ncounter = 0\n#training anf validating\nfor in_center, in_context, out_label in tfdataset_gen:\n    #train step\n    loss_, gradients = train_step(in_center, in_context, out_label, loss_function)\n    #adding loss to train loss\n    train_loss(loss_)\n\n    counter = counter + 1\n          \n    ##tensorboard \n    with tf.name_scope('per_step_training'):\n        with wtrain.as_default():\n            tf.summary.scalar(\"batch_loss\", loss_, step=counter)\n    with tf.name_scope(\"per_batch_gradients\"):\n        with wtrain.as_default():\n            for i in range(len(sgns_w2v.trainable_variables)):\n                name_temp = sgns_w2v.trainable_variables[i].name\n                tf.summary.histogram(name_temp, gradients[i], step=counter)\n    \n    if counter%100 == 0:\n        #printing\n        template = '''Done {} iterations, Loss: {:0.6f}'''\n    \n        print(template.format(counter, train_loss.result()))\n\n        if counter%200 == 0:\n            ckpt_save_path  = ckpt_manager.save()\n            print ('Saving checkpoint for iteration {} at {}'.format(counter+1, ckpt_save_path))\n        \n        train_loss.reset_states()\n    if counter &gt; no_iterations:\n        break\n\nYou can check total code and results in my GitHub link below.\n\ngithub: https://github.com/UdiBhaskar/Natural-Language-Processing/blob/master/Feature%20Extraction%20Methods/Advanced%20feature%20extraction%20-%20W2V/W2V_Tensorflow_Negative_Sampling.ipynb\n\nSaved the model into gensim Word2Vec format and loaded\n\nsave_word2vec_format_dict(binary=True, fname='w2vns.bin', total_vec=len(word_vectors_dict), vocab=model_gensim.vocab, vectors=model_gensim.vectors)\nmodel_gensim = gensim.models.keyedvectors.Word2VecKeyedVectors.load_word2vec_format('w2vns.bin', binary=True)\n\n\nImportant: Negative Sampling is a simplified version of Noise Contrastive Estimation. NCE guarantees approximation to softmax, Negative Sampling doesn’t. You can read this in paper/blog.\n\n\n\n\nWord2Vec using Tensorflow (Skip-Gram, NCE)\nLet’s take a which gives the score to each pair of the skip-grams, we will try to maximize the (score of positive pairs to the word - score of negative pairs) to the word. We can do that directly by optimizing the tf.nn.nce_loss. Please try to read the documentation. It takes a positive pair, weight vectors and then generates the negative pairs based on sampled_values, and gives the loss.\n\nPreparing the Data\nWe have to generate a positive pair of skip-grams, we can do it in a similar way as above. Created a pipeline to generate batchwise data as below.\n\n##getting sentence wise data\nlist_sents = [nltk.word_tokenize(sent) for sent_tok in data_imdb.review for sent in nltk.sent_tokenize(sent_tok)]\n##to use tf.keras.preprocessing.sequence.skipgrams, we have to encode our sentence to numbers. so used Tokenizer class\ntokenizer = tf.keras.preprocessing.text.Tokenizer()\ntokenizer.fit_on_texts(list_sents)\nseq_texts = tokenizer.texts_to_sequences(list_sents) ##list of list\n\ndef generate_sgns():\n    for seq in seq_texts:\n        generated_samples, labels = tf.keras.preprocessing.sequence.skipgrams(sequence=seq, \n                                                                      vocabulary_size=len(tokenizer.word_index)+1, \n                                                                      window_size=2, negative_samples=0)\n        length_samples = len(generated_samples)\n        for i in range(length_samples):\n            yield [generated_samples[i][0]], [generated_samples[i][1]]\n\n##creating the tf dataset\ntfdataset_gen = tf.data.Dataset.from_generator(generate_sgns, output_types=(tf.int64, tf.int64))\ntfdataset_gen = tfdataset_gen.repeat().batch(1024).prefetch(tf.data.experimental.AUTOTUNE)\n\n\n\nCreating Model\nI created a model word2vecNCS which takes a center word, context word and give NCE loss. You can check that below.\n\nclass word2vecNCS(Model):\n    def __init__(self, vocab_size, embed_size, num_sampled, **kwargs):\n        '''NCS Word2Vec\n        vocab_size: Size of vocabulary you have\n        embed_size: Embedding size needed\n        num_sampled: No of negative sampled to generate'''\n        super(word2vecNCS, self).__init__(**kwargs)\n        self.vocab_size = vocab_size\n        self.embed_size = embed_size\n        self.num_sampled = num_sampled\n        ##embedding layer\n        self.embed_layer = Embedding(input_dim=vocab_size, output_dim=embed_size,embeddings_initializer=tf.keras.initializers.RandomUniform(seed=32))\n        ##reshing layer\n        self.reshape_layer = Reshape((self.embed_size,))\n    def build(self, input_shape):\n        ##weights needed for nce loss\n        self.nce_weight = self.add_weight(shape=(self.vocab_size, self.embed_size),\n                             initializer=tf.keras.initializers.TruncatedNormal(mean=0, stddev= (1/self.embed_size**0.5)),\n                             trainable=True, name=\"nce_weight\")\n        #biases needed nce loss\n        self.nce_bias = self.add_weight(shape=(self.vocab_size), initializer=\"zeros\", trainable=True, name=\"nce_bias\")\n\n    def call(self, input_center_word, input_context_word):\n        '''\n        input_center_word: center word\n        input_context_word: context word''' \n        ##giving center word and getting the embedding\n        embedd_out = self.embed_layer(input_center_word)\n        ##rehaping \n        embedd_out = self.reshape_layer(embedd_out)\n        ##calculating nce loss\n        nce_loss = tf.reduce_sum(tf.nn.nce_loss(weights=self.nce_weight, \n                                  biases=self.nce_bias, \n                                  labels=input_context_word, \n                                  inputs=embedd_out, \n                                  num_sampled=self.num_sampled, \n                                  num_classes=self.vocab_size))\n        return nce_loss\n\n\n\nTraining\n\n##training\n\n##optimizer\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n\nsgncs_w2v = word2vecNCS(len(tokenizer.word_index)+1, 100, 32, name=\"w2vNCE\")\n\n##train step function to train\n@tf.function\ndef train_step(input_center, input_context):\n    with tf.GradientTape() as tape:\n        #forward propagation\n        loss = sgncs_w2v(input_center, input_context)\n\n    #getting gradients\n    gradients = tape.gradient(loss, sgncs_w2v.trainable_variables)\n    #applying gradients\n    optimizer.apply_gradients(zip(gradients, sgncs_w2v.trainable_variables))\n\n    return loss, gradients\n\n##number of epochs\nno_iterations=10000\n\n##metrics # Even if you use .fit method, it alsocalculates batchwise loss/metric and aggregates those.  \ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\n\n#tensorboard file writers\nwtrain = tf.summary.create_file_writer(logdir='/content/drive/My Drive/word2vec/logs/w2vncs/train')\n\n##check point to save\ncheckpoint_path = \"/content/drive/My Drive/word2vec/checkpoints/w2vNCS/train\"\nckpt = tf.train.Checkpoint(optimizer=optimizer, model=sgncs_w2v)\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n\n\ncounter = 0\n#training anf validating\nfor in_center, in_context in tfdataset_gen:\n    #train step\n    loss_, gradients = train_step(in_center, in_context)\n    #adding loss to train loss\n    train_loss(loss_)\n\n    counter = counter + 1\n         \n    ##tensorboard \n    with tf.name_scope('per_step_training'):\n        with wtrain.as_default():\n            tf.summary.scalar(\"batch_loss\", loss_, step=counter)\n    with tf.name_scope(\"per_batch_gradients\"):\n        with wtrain.as_default():\n            for i in range(len(sgncs_w2v.trainable_variables)):\n                name_temp = sgncs_w2v.trainable_variables[i].name\n                tf.summary.histogram(name_temp, gradients[i], step=counter)\n    \n    if counter%100 == 0:\n        #printing\n        template = '''Done {} iterations, Loss: {:0.6f}'''\n    \n        print(template.format(counter, train_loss.result()))\n\n        if counter%200 == 0:\n            ckpt_save_path  = ckpt_manager.save()\n            print ('Saving checkpoint for iteration {} at {}'.format(counter+1, ckpt_save_path))\n        \n        train_loss.reset_states()\n    if counter &gt; no_iterations :\n        break\n\nYou can check total code and results in my GitHub link below.\n\ngithub: https://github.com/UdiBhaskar/Natural-Language-Processing/blob/master/Feature%20Extraction%20Methods/Advanced%20feature%20extraction%20-%20W2V/W2V_Tensorflow_NCE.ipynb"
  },
  {
    "objectID": "notebooks/Advanced-Feature-Extraction.html#fast-text-embedding-sub-word-embedding",
    "href": "notebooks/Advanced-Feature-Extraction.html#fast-text-embedding-sub-word-embedding",
    "title": "Advanced Feature Extraction from Text",
    "section": "Fast-text Embedding (Sub-Word Embedding)",
    "text": "Fast-text Embedding (Sub-Word Embedding)\nInstead of feeding individual words into the Neural Network, FastText breaks words into several n-grams (sub-words). For instance, tri-grams for the word where is &lt;wh, whe, her, ere, re&gt; and the special sequence &lt;where&gt;. Note that the sequence, corresponding to the word her is different from the tri-gram her from the word where. Because of these subwords, we can get embedding for any word we have even it is a misspelled word. Try to read this paper.\n\nWe can train these vectors using the gensim or fastText official implementation. Trained fastText word embedding with gensim, you can check that below. It’s a single line of code similar to Word2vec.\n\n##FastText module\nfrom gensim.models import FastText\ngensim_fasttext = FastText(sentences=list_sents, \n                           sg=1, ##skipgram\n                           hs=0, #negative sampling \n                           min_count=4, ##min count of any vocab \n                           negative=10, ##no of negative samples \n                           iter=15, ##no of iterations\n                           size=100, ##dimentions of word\n                           window=3, ##window size to get the skipgrams\n                           seed=34)\n\nYou can get the total code in the below GitHub\n\ngithub: https://github.com/UdiBhaskar/Natural-Language-Processing/blob/master/Feature%20Extraction%20Methods/Advanced%20feature%20extraction%20-%20W2V/fasttext_Training.ipynb"
  },
  {
    "objectID": "notebooks/Advanced-Feature-Extraction.html#pre-trained-word-embedding",
    "href": "notebooks/Advanced-Feature-Extraction.html#pre-trained-word-embedding",
    "title": "Advanced Feature Extraction from Text",
    "section": "Pre-Trained Word Embedding",
    "text": "Pre-Trained Word Embedding\nWe can get pre-trained word embedding that was trained on huge data by Google, Stanford NLP, Facebook.\n\nGoogle Word2Vec\nYou can download google’s pretrained wordvectors trained on Google news data from this link. You can load the vectors as gensim model like below\n\ngooglew2v_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n\n\n\nGloVe Pretrained Embeddings\nYou can download the glove embedding from this link. There are some differences between Google Word2vec save format and GloVe save format. We can convert Glove format to google format and then load that using gensim as below.\n\nfrom gensim.scripts.glove2word2vec import glove2word2vec\nglove2word2vec(glove_input_file=\"glove.42B.300d.txt\", word2vec_output_file=\"w2vstyle_glove_vectors.txt\")\n\nglove_model = gensim.models.KeyedVectors.load_word2vec_format(\"w2vstyle_glove_vectors.txt\", binary=False)\n\n\n\nFastText Pretrained Embeddings\nYou can get the fasttext word embeedings from this link. You can use fasttext python api or gensim to load the model. I am using gensim.\n\nfrom gensim.models import FastText\nfasttext_model = FastText.load_fasttext_format(\"/content/cc.en.300.bin\")\n\n\nReferences:\n\ngensim documentation\nhttps://fasttext.cc/\nCS7015 - IIT Madras\nhttps://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html\nhttps://arxiv.org/abs/1410.8251\nhttps://ruder.io/word-embeddings-softmax/"
  },
  {
    "objectID": "notebooks/Effective_Training_and_Debugging_of_a_Neural_Networks.html",
    "href": "notebooks/Effective_Training_and_Debugging_of_a_Neural_Networks.html",
    "title": "Effective Training and Debugging of a Neural Networks",
    "section": "",
    "text": "In this blog, I want to discuss Training and Debugging of a NN in a practical manner.\nI am taking a cyber troll dataset. It is a classification data with labels aggressive or not. This is mostly inspired from this blog.\nWhenever I train any neural network, I will divide that into subtasks as below. I am assuming, you already set your project goals and evaluation metrics.\n##basic imports\nimport numpy as np\nimport pandas as pd\nimport random as rn\nimport tensorflow as tf\nfrom tensorflow.keras.layers import LSTM, GRU, Dense, Input, Embedding\nfrom tensorflow.keras.models import Model"
  },
  {
    "objectID": "notebooks/Effective_Training_and_Debugging_of_a_Neural_Networks.html#data-processing",
    "href": "notebooks/Effective_Training_and_Debugging_of_a_Neural_Networks.html#data-processing",
    "title": "Effective Training and Debugging of a Neural Networks",
    "section": "Data Processing",
    "text": "Data Processing\n\n##reading the data\ncyber_troll_data = pd.read_json('Dataset for Detection of Cyber-Trolls.json', lines=True)\ncyber_troll_data.head(2)\n\n\n\n\n\n\n\n\ncontent\nannotation\nextras\nmetadata\n\n\n\n\n0\nGet fucking real dude.\n{'notes': '', 'label': ['1']}\nNaN\n{'first_done_at': 1527503426000, 'last_updated...\n\n\n1\nShe is as dirty as they come and that crook R...\n{'notes': '', 'label': ['1']}\nNaN\n{'first_done_at': 1527503426000, 'last_updated...\n\n\n\n\n\n\n\n\n#basic preprocessing\ncyber_troll_data['label']=cyber_troll_data.annotation.apply(lambda x: int(x['label'][0]))\ncyber_troll_data = cyber_troll_data[['content', 'label']]\ncyber_troll_data.head()\n\n\n\n\n\n\n\n\ncontent\nlabel\n\n\n\n\n0\nGet fucking real dude.\n1\n\n\n1\nShe is as dirty as they come and that crook R...\n1\n\n\n2\nwhy did you fuck it up. I could do it all day ...\n1\n\n\n3\nDude they dont finish enclosing the fucking sh...\n1\n\n\n4\nWTF are you talking about Men? No men thats no...\n1\n\n\n\n\n\n\n\n\n#its a imbalance one\ncyber_troll_data.label.value_counts()\n\n0    12179\n1     7822\nName: label, dtype: int64\n\n\n\n##splitting data into train, validation and Test data. \nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(cyber_troll_data.content, cyber_troll_data.label, \n                                                    test_size=0.40, stratify=cyber_troll_data.label, random_state=54)\n\nX_test, X_val, y_test, y_val = train_test_split(X_test, y_test, \n                                                    test_size=0.50, stratify=y_test, random_state=32)\n\n\ntokenizer = tf.keras.preprocessing.text.Tokenizer()\ntokenizer.fit_on_texts(X_train)\n\n\nX_train_tokens = tokenizer.texts_to_sequences(X_train)\nX_test_tokens = tokenizer.texts_to_sequences(X_test)\nX_val_tokens = tokenizer.texts_to_sequences(X_val)\n\n\nnumber_vocab = len(tokenizer.word_index)+1\n\nX_train_pad_tokens = tf.keras.preprocessing.sequence.pad_sequences(X_train_tokens, maxlen=24, padding='post', truncating='post')\nX_test_pad_tokens = tf.keras.preprocessing.sequence.pad_sequences(X_test_tokens, maxlen=24, padding='post', truncating='post')\nX_val_pad_tokens = tf.keras.preprocessing.sequence.pad_sequences(X_val_tokens, maxlen=24, padding='post', truncating='post')\n\nWe prepared the data. I am not doing perfect preprocessing and tokenization. You can do preprocessing in a better way.\nWe have,\nX_train_pad_tokens, y_train  --&gt; To train   X_val_pad_tokens, y_val   --&gt; To validate and Tune   X_test_pad_tokens, y_test  --&gt; Don't use this data while trainig. Only use this after you are done with all the modelling.\nI am creating Training and Validation datasets to iterate over those using the tf.data pipeline. Please use less data as of now because, it will be easier to debug and easier to know about the error if we have any in our network, I will discuss this below. I am only using the first 100 data points with a batch size of 32.\n\n##Creating the dataset( only 100 data points and will explain why after trainig process.) \ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train_pad_tokens[0:100], y_train[0:100]))\ntrain_dataset = train_dataset.shuffle(1000).batch(32, drop_remainder=True)\ntrain_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n\n##creating test dataset using tf.data\nval_dataset = tf.data.Dataset.from_tensor_slices((X_val_pad_tokens[0:100], y_val[0:100]))\nval_dataset = val_dataset.batch(32, drop_remainder=True)\nval_dataset = val_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n\n\nChecking the data pairing issue and check the data given to neural network is correct or not. If it got corrupted, check/debug the data pipleline and rectify it. If you have images, try to plot the images and check.\nbelow, i have written a basic for loop to print. You can also print the words corresponding to the numbers and check.\n\nfor input_text, output_label in train_dataset:\n    print(input_text[0:3], output_label[0:3])\n    break\n\ntf.Tensor(\n[[ 186   89  741    5  385   43   11  127  919 1082  157    1    9  251\n     5  628    3 6970    5   11 4641   30    6   40]\n [  27    3   26   28 1021   29    6    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0]\n [4647   72  606   43   16  684  223    1    9    3 4648  923    0    0\n     0    0    0    0    0    0    0    0    0    0]], shape=(3, 24), dtype=int32) tf.Tensor([0 1 1], shape=(3,), dtype=int32)"
  },
  {
    "objectID": "notebooks/Effective_Training_and_Debugging_of_a_Neural_Networks.html#creating-a-neural-network",
    "href": "notebooks/Effective_Training_and_Debugging_of_a_Neural_Networks.html#creating-a-neural-network",
    "title": "Effective Training and Debugging of a Neural Networks",
    "section": "Creating a Neural Network",
    "text": "Creating a Neural Network\nSome of the rules to follow while writing/training your Neural Network.\n\nStart with a simple architecture - We are doing a text classification so, we can try a single layer LSTM.\nUse well studied default parameters like activation = relu, optimizer = adam, initialization = he for relu and Glorot for sigmoid/tanh. To know more about this, please read this blog.\nFix the random seeds so that we can reproduce the initializations/results to tune our models. - You have to fix all the random seeds in your model.\nNormalize the input data.\n\nI am writing a simple LSTM model by following all the above rules.\n\n##LSTM\n\n##fixing numpy RS\nnp.random.seed(42)\n\n##fixing tensorflow RS\ntf.random.set_seed(32)\n\n##python RS\nrn.seed(12)\n\n\n##model\ndef get_model():\n    input_layer = Input(shape=(24,), name=\"input_layer\")\n    ##i am initilizing randomly. But you can use predefined embeddings. \n    x_embedd = Embedding(input_dim=number_vocab, output_dim=100, input_length=24, mask_zero=True, \n                        embeddings_initializer=tf.keras.initializers.RandomNormal(mean=0, stddev=1, seed=23),\n                         name=\"Embedding_layer\")(input_layer)\n    \n    x_lstm = LSTM(units=20, activation='tanh', recurrent_activation='sigmoid', use_bias=True, \n                 kernel_initializer=tf.keras.initializers.glorot_uniform(seed=26),\n                 recurrent_initializer=tf.keras.initializers.orthogonal(seed=54),\n                 bias_initializer=tf.keras.initializers.zeros(), name=\"LSTM_layer\")(x_embedd)\n    \n    x_out = Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.glorot_uniform(seed=45),\n                  name=\"output_layer\")(x_lstm)\n    \n    basic_lstm_model = Model(inputs=input_layer, outputs=x_out, name=\"basic_lstm_model\")\n    \n    return basic_lstm_model\n\n\nbasic_lstm_model = get_model()\nbasic_lstm_model_anothertest = get_model()\n\n Now i created two models named basic_lstm_model, basic_lstm_model_anothertest. Those two model initial weights will be the same because of the fixed random seed. This removes a factor of a variation and very useful to tune parameters by doing some experimentation on the same weight initialization.\nwe can check this as below.\n\n[np.all(basic_lstm_model.get_weights()[i]==basic_lstm_model_anothertest.get_weights()[i]) \\\n for i in range(len(basic_lstm_model.get_weights()))]\n\n[True, True, True, True, True, True]"
  },
  {
    "objectID": "notebooks/Effective_Training_and_Debugging_of_a_Neural_Networks.html#training-a-nn",
    "href": "notebooks/Effective_Training_and_Debugging_of_a_Neural_Networks.html#training-a-nn",
    "title": "Effective Training and Debugging of a Neural Networks",
    "section": "Training a NN",
    "text": "Training a NN\nLoss functions - If we calculate the loss in the wrong manner, we will get the wrong gradients and it doesn’t learn perfectly.\nSome of the mistakes in Loss functions:\n\none of the main mistakes in the loss creation is giving wrong inputs to the loss function. If we are using the cross-entropy, you have to give one-hot vector as input otherwise, use sparse_categorical_crossentropy(no need to give the one-hot vectors).\nIf you are using a function that calculates the loss using unnormalized logits, don’t give the probability output as input to the loss function. ( check logits parameter in the tensorflow loss functions)\nIt is useful to mask unnecessary output while calculating loss. Eg: don’t include output at the padded word position while calculation loss.\nSelecting a loss function that allowing the calculation of large error values. Because of this, your loss may explode, you may get NaN and it affects the gradients too.\n\n\n##masked loss Eg for sequence output. \ndef maskedLoss(y_true, y_pred):\n    #getting mask value\n    mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n    \n    #calculating the loss\n    loss_ = loss_function(y_true, y_pred)\n    \n    #converting mask dtype to loss_ dtype\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    \n    #applying the mask to loss\n    loss_ = loss_*mask\n    \n    #getting mean over all the values\n    loss_ = tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n    return loss_\n\n\n##creating a loss object for this classification problem\nloss_function = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction='auto')\n\nTraining and validation functions\n\nWe have to take care of the toggling training flag because some of the layers behaves differently in training and testing.\n\n\n#optimizer\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n\n#trainign function\n@tf.function\ndef train_step(input_vector, output_vector,loss_fn):\n    #taping the gradients\n    with tf.GradientTape() as tape:\n        #for ward prop\n        output_predicted = basic_lstm_model(inputs=input_vector, training=True)\n        #loss calculation\n        loss = loss_fn(output_vector, output_predicted)\n    #getting gradients\n    gradients = tape.gradient(loss, basic_lstm_model.trainable_variables)\n    #applying gradients\n    optimizer.apply_gradients(zip(gradients, basic_lstm_model.trainable_variables))\n    return loss, output_predicted\n\n#validation function\n@tf.function\ndef val_step(input_vector, output_vector, loss_fn):\n    #forward prop\n    output_predicted = basic_lstm_model(inputs=input_vector, training=False)\n    #loss calculation\n    loss = loss_fn(output_vector, output_predicted)\n    return loss, output_predicted\n\nTraining the NN with proper data.\n\nWhile Training the model, I suggest you don't write the complex pipelining of the data and train your network at the start. If you do this, finding the bugs in your network is very difficult. Just get a few instances of data( maybe 10% of your total train data if you have 10K records) into your RAM and try to train your network. In this case, I have total data in my RAM so, I will slice a few batches and try to train the network.\nI will suggest you don't include the data augmentation as of now. It is useful for regularizing the model but try to avoid it at the start. Even if you do data augmentation, be careful about the labels. Eg: In the segmentation task, if you flip the image, you have to flip the label image as well.\nCheck for casting issues. Eg. If layer needs int8, give the int8 value only as input. If you have float values, just cast the dtype. If data stored in the disk is float32, load the data into RAM with the same dtype.\nCheck the data pairing issue i.e. while giving the train data, you have to give the correct pairs of x and y. Training the NN with proper data.\n\nTraining the NN with data for 2 epochs and printing batchwise loss and finally getting mean of all those. Even if you use the .fit method of Keras API, it prints the aggregated value of loss/metric as part of verbose. You can check that aggregate class here\n\n##training\nEPOCHS=2\n\n##metrics # Even if you use .fit method, it alsocalculates batchwise loss/metric and aggregates those.  \ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\nval_loss = tf.keras.metrics.Mean(name='test_loss')\n\nfor epoch in range(EPOCHS):\n    #losses\n    train_loss.reset_states()\n    val_loss.reset_states()\n    \n    #training\n    print('Batchwise Train loss')\n    for text_seq, label_seq in train_dataset:\n        loss_, pred_out = train_step(text_seq, label_seq, loss_function)\n        print(loss_)\n        train_loss(loss_)\n    \n    #validation\n    print('Batchwise Val loss')\n    for text_seq_val, label_seq_val in val_dataset:\n        loss_test, pred_out_test = val_step(text_seq_val, label_seq_val, loss_function)\n        print(loss_test)\n        val_loss(loss_test)\n    \n    template = 'Epoch {}, Mean Loss: {}, Mean Val Loss: {}'\n    \n    print(template.format(epoch+1, train_loss.result(), val_loss.result()))\n    print('-'*50)\n\nBatchwise Train loss\ntf.Tensor(0.69066906, shape=(), dtype=float32)\ntf.Tensor(0.6978342, shape=(), dtype=float32)\ntf.Tensor(0.7214557, shape=(), dtype=float32)\nBatchwise Val loss\ntf.Tensor(0.7479876, shape=(), dtype=float32)\ntf.Tensor(0.6868224, shape=(), dtype=float32)\ntf.Tensor(0.71952724, shape=(), dtype=float32)\nEpoch 1, Mean Loss: 0.7033197283744812, Mean Val Loss: 0.7181124687194824\n--------------------------------------------------\nBatchwise Train loss\ntf.Tensor(0.6816538, shape=(), dtype=float32)\ntf.Tensor(0.69258916, shape=(), dtype=float32)\ntf.Tensor(0.6689039, shape=(), dtype=float32)\nBatchwise Val loss\ntf.Tensor(0.744266, shape=(), dtype=float32)\ntf.Tensor(0.681653, shape=(), dtype=float32)\ntf.Tensor(0.71762204, shape=(), dtype=float32)\nEpoch 2, Mean Loss: 0.6810489296913147, Mean Val Loss: 0.7145137190818787\n--------------------------------------------------"
  },
  {
    "objectID": "notebooks/Effective_Training_and_Debugging_of_a_Neural_Networks.html#debugging-and-enhancing-nn",
    "href": "notebooks/Effective_Training_and_Debugging_of_a_Neural_Networks.html#debugging-and-enhancing-nn",
    "title": "Effective Training and Debugging of a Neural Networks",
    "section": "Debugging and Enhancing NN",
    "text": "Debugging and Enhancing NN\nTill now, we have created a basic NN for our problem and trained the NN. Now I will discuss some hacks to debug and enhance your training process to get better results.\n\nUsing Basic print statements and checking the shapes of input and output of every layer. Using this, we can remove the shape related error or basic errors related to output while creating a model. If you want to print in tensorflow code, please use tf.print\nWith Eager execution, we can debug our code very easily. it can be done using pdb or using any ide. You have to set tf.config.experimental_run_functions_eagerly(True) to debug your tf2.0 functions.\n\n\n##LSTM\n\ntf.config.experimental_run_functions_eagerly(True)\n\n##fixing numpy RS\nnp.random.seed(42)\n\n##fixing tensorflow RS\ntf.random.set_seed(32)\n\n##python RS\nrn.seed(12)\n\nimport pdb\n\n##model\ndef get_model_debug():\n    input_layer_d = Input(shape=(24,), name=\"input_layer\")\n    ##i am initilizing randomly. But you can use predefined embeddings. \n    x_embedd_d= Embedding(input_dim=number_vocab, output_dim=100, input_length=24, mask_zero=True, \n                        embeddings_initializer=tf.keras.initializers.RandomNormal(mean=0, stddev=1, seed=23),\n                         name=\"Embedding_layer\")(input_layer_d)\n    \n    #LSTM\n    x_lstm_d = LSTM(units=20, activation='tanh', recurrent_activation='sigmoid', use_bias=True, \n                 kernel_initializer=tf.keras.initializers.glorot_uniform(seed=26),\n                 recurrent_initializer=tf.keras.initializers.orthogonal(seed=54),\n                 bias_initializer=tf.keras.initializers.zeros(), name=\"LSTM_layer\")(x_embedd_d)\n    \n    #trace\n    pdb.set_trace()\n    \n    x_out_d = Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.glorot_uniform(seed=45),\n                  name=\"output_layer\")(x_lstm_d)\n    \n    basic_lstm_model_d = Model(inputs=input_layer_d, outputs=x_out_d, name=\"basic_lstm_model_d\")\n    \n    return basic_lstm_model_d\n\n\nbasic_model_debug = get_model_debug()\n\ntf.config.experimental_run_functions_eagerly(False)\n\n&gt; &lt;ipython-input-14-476c66b41633&gt;(31)get_model_debug()\n-&gt; x_out_d = Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.glorot_uniform(seed=45),\n(Pdb)  locals()\n{'input_layer_d': &lt;tf.Tensor 'input_layer_2:0' shape=(None, 24) dtype=float32&gt;, 'x_embedd_d': &lt;tf.Tensor 'Embedding_layer_2/Identity:0' shape=(None, 24, 100) dtype=float32&gt;, 'x_lstm_d': &lt;tf.Tensor 'LSTM_layer_2/Identity:0' shape=(None, 20) dtype=float32&gt;}\n(Pdb)  n\n&gt; &lt;ipython-input-14-476c66b41633&gt;(32)get_model_debug()\n-&gt; name=\"output_layer\")(x_lstm_d)\n(Pdb)  n\n&gt; &lt;ipython-input-14-476c66b41633&gt;(34)get_model_debug()\n-&gt; basic_lstm_model_d = Model(inputs=input_layer_d, outputs=x_out_d, name=\"basic_lstm_model_d\")\n(Pdb)  c\n\n\nYou can also Debug the Trainig loopas shown below.\nFor PDB instrctions, please check this PDF.\nMy preference and suggestion is to use IDE Debugger\n\n##training\nEPOCHS=1\n##metrics # Even if you use .fit method, it alsocalculates batchwise loss/metric and aggregates those.  \ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\n\ntf.config.experimental_run_functions_eagerly(True)\nfor epoch in range(EPOCHS):\n    train_loss.reset_states()\n    \n    print('Batchwise Train loss')\n    for text_seq, label_seq in train_dataset:\n        pdb.set_trace()\n        loss_, pred_out = train_step(text_seq, label_seq, loss_function)\n        print(loss_)\n        train_loss(loss_)\n    \n    template = 'Epoch {}, Mean Loss: {}'\n    \n    print(template.format(epoch+1, train_loss.result()))\n    print('-'*50)\ntf.config.experimental_run_functions_eagerly(False)\n\nBatchwise Train loss\n&gt; &lt;ipython-input-15-aa3750dbfb83&gt;(13)&lt;module&gt;()\n-&gt; loss_, pred_out = train_step(text_seq, label_seq, loss_function)\n(Pdb)  s\n--Call--\n&gt; d:\\softwares\\anaconda3\\envs\\tf2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py(551)__call__()\n-&gt; def __call__(self, *args, **kwds):\n(Pdb)  c\ntf.Tensor(0.66431165, shape=(), dtype=float32)\n&gt; &lt;ipython-input-15-aa3750dbfb83&gt;(12)&lt;module&gt;()\n-&gt; pdb.set_trace()\n(Pdb)  c\ntf.Tensor(0.6668887, shape=(), dtype=float32)\n&gt; &lt;ipython-input-15-aa3750dbfb83&gt;(13)&lt;module&gt;()\n-&gt; loss_, pred_out = train_step(text_seq, label_seq, loss_function)\n(Pdb)  c\ntf.Tensor(0.6523603, shape=(), dtype=float32)\nEpoch 1, Mean Loss: 0.6611868739128113\n--------------------------------------------------\n\n\n\nOnce you are done with the creation of the model, Try to Train the model with less data( i have taken 100 samples) and try to overfit the model to that data. To do so we increase the capacity of our model (e.g. add layers or filters) and verify that we can reach the lowest achievable loss (e.g. zero). If your model is unable to overfit a few data points, then either it’s too small (which is unlikely in today’s age), or something is wrong in its structure or the learning algorithm. check for bugs and try to remove those. I will discuss some of the bugs below. If this model is working fine without any bugs, you can train with full data.\nTensorboard is another important tool to debug NN while training. You can visualize the Loss, metrics, gradient/output histograms, distributions, graph and many more. I am writing code to plot all these in the tensorboard.\nAs of now, we are printing/plotting the Mean loss/metric for all the batches in one epoch and, based on this we are analyzing the model performance. This may lead to wrong models for some of the loss functions/metrics. Even if you use the smoothing, it is not an accurate one, it will get an exponentially weighted average over batch-wise loss/metric. so Try to get a loss/metric for entire data of train and Val/test. If you have time/space constraint, at least get for the val/test data. Eg: Mean of Cross entropy over batches is equal to the cross-entropy over total data but not for AUC/F1 score.\nBelow I have written code that calculates loss and metric(AUC) over batches and gets the mean as well as a total loss at once and a better Training and validation functions with tensorboard. please look into it.\n\n\n##training\n\n##model creation\nbasic_lstm_model = get_model()\n\n##optimizer\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n\n##metric\nfrom sklearn.metrics import roc_auc_score\n\n##train step function to train\n@tf.function\ndef train_step(input_vector, output_vector,loss_fn):\n    with tf.GradientTape() as tape:\n        #forward propagation\n        output_predicted = basic_lstm_model(inputs=input_vector, training=True)\n        #loss\n        loss = loss_fn(output_vector, output_predicted)\n    #getting gradients\n    gradients = tape.gradient(loss, basic_lstm_model.trainable_variables)\n    #applying gradients\n    optimizer.apply_gradients(zip(gradients, basic_lstm_model.trainable_variables))\n    return loss, output_predicted, gradients\n\n##validation step function\n@tf.function\ndef val_step(input_vector, output_vector, loss_fn):\n    #getting output of validation data\n    output_predicted = basic_lstm_model(inputs=input_vector, training=False)\n    #loss calculation\n    loss = loss_fn(output_vector, output_predicted)\n    return loss, output_predicted\n\nimport math\n\n#batch size\nBATCH_SIZE=32\n##number of epochs\nEPOCHS=10\n\n##metrics # Even if you use .fit method, it alsocalculates batchwise loss/metric and aggregates those.  \ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\nval_loss = tf.keras.metrics.Mean(name='val_loss')\ntrain_metric = tf.keras.metrics.Mean(name=\"train_auc\")\nval_metric = tf.keras.metrics.Mean(name=\"val_metric\")\n\n#tensorboard file writers\nwtrain = tf.summary.create_file_writer(logdir='logs\\\\train')\nwval = tf.summary.create_file_writer(logdir='logs\\\\val')\n\n\n#no of data points/batch_size i.e number of iterations in the one epoch\niters = math.ceil(100/BATCH_SIZE) \n\n#training anf validating\nfor epoch in range(EPOCHS):\n    \n    #resetting the states of the loss and metrics\n    train_loss.reset_states()\n    val_loss.reset_states()\n    train_metric.reset_states()\n    val_metric.reset_states()\n    \n    ##counter for train loop iteration\n    counter = 0\n    \n    #lists to save true and validation data. \n    train_true = []\n    train_predicted = []\n    val_true = []\n    val_predicted = []\n    \n    #ietrating over train data batch by batch\n    for text_seq, label_seq in train_dataset:\n        #train step\n        loss_, pred_out, gradients = train_step(text_seq, label_seq, loss_function)\n        #adding loss to train loss\n        train_loss(loss_)\n        #counting the step number\n        temp_step = epoch*iters+counter\n        counter = counter + 1\n        \n        #calculating AUC for batch\n        batch_metric = roc_auc_score(label_seq, pred_out)\n        train_metric(batch_metric)\n        \n        #appending it to list\n        train_predicted.append(pred_out)\n        train_true.append(label_seq)\n        \n        ##tensorboard \n        with tf.name_scope('per_step_training'):\n            with wtrain.as_default():\n                tf.summary.scalar(\"batch_loss\", loss_, step=temp_step)\n                tf.summary.scalar('batch_metric', batch_metric, step=temp_step)\n        with tf.name_scope(\"per_batch_gradients\"):\n            with wtrain.as_default():\n                for i in range(len(basic_lstm_model.trainable_variables)):\n                    name_temp = basic_lstm_model.trainable_variables[i].name\n                    tf.summary.histogram(name_temp, gradients[i], step=temp_step)\n    \n    #calculating the final loss and metric\n    train_true = tf.concat(train_true, axis=0)\n    train_predicted = tf.concat(train_predicted, axis=0)\n    train_loss_final = loss_function(train_true, train_predicted)\n    train_metric_auc = roc_auc_score(train_true, train_predicted)\n    \n    #validation data\n    for text_seq_val, label_seq_val in val_dataset:\n        #getting val output\n        loss_val, pred_out_val = val_step(text_seq_val, label_seq_val, loss_function)\n        #appending to lists\n        val_true.append(label_seq_val)\n        val_predicted.append(pred_out_val)\n        val_loss(loss_val)\n        \n        #calculating metric\n        batch_metric_val = roc_auc_score(label_seq_val, pred_out_val)\n        val_metric(batch_metric_val)\n    \n    \n    #calculating final loss and metric   \n    val_true = tf.concat(val_true, axis=0)\n    val_predicted = tf.concat(val_predicted, axis=0)\n    val_loss_final = loss_function(val_true, val_predicted)\n    val_metric_auc = roc_auc_score(val_true, val_predicted)\n    \n    #printing\n    template = '''Epoch {}, Train Loss: {:0.6f}, Mean batch Train Loss: {:0.6f}, AUC: {:0.5f}, Mean batch Train AUC: {:0.5f},\n    Val Loss: {:0.6f}, Mean batch Val Loss: {:0.6f}, Val AUC: {:0.5f}, Mean batch Val AUC: {:0.5f}'''\n    \n    print(template.format(epoch+1, train_loss_final.numpy(), train_loss.result(), \n                          train_metric_auc, train_metric.result(), val_loss_final.numpy(),\n                          val_loss.result(), val_metric_auc, val_metric.result()))\n    print('-'*30)\n    \n    #tensorboard\n    with tf.name_scope(\"per_epoch_loss_metric\"):\n        with wtrain.as_default():\n            tf.summary.scalar(\"mean_loss\", train_loss.result().numpy(), step=epoch)\n            tf.summary.scalar('loss', train_loss_final.numpy(), step=epoch)\n            tf.summary.scalar('metric', train_metric_auc, step=epoch)\n            tf.summary.scalar('mean_metric', train_metric.result().numpy(), step=epoch)\n        with wval.as_default():\n            tf.summary.scalar('mean_loss', val_loss.result().numpy(), step=epoch)\n            tf.summary.scalar('loss', val_loss_final.numpy(), step=epoch)\n            tf.summary.scalar('metric', val_metric_auc, step=epoch)\n            tf.summary.scalar('mean_metric', val_metric.result().numpy(), step=epoch)\n\nEpoch 1, Train Loss: 0.700775, Mean batch Train Loss: 0.700775, AUC: 0.46829, Mean batch Train AUC: 0.45378,\n    Val Loss: 0.704532, Mean batch Val Loss: 0.704532, Val AUC: 0.48223, Mean batch Val AUC: 0.48844\n------------------------------\nEpoch 2, Train Loss: 0.596350, Mean batch Train Loss: 0.596350, AUC: 0.86608, Mean batch Train AUC: 0.86355,\n    Val Loss: 0.691127, Mean batch Val Loss: 0.691127, Val AUC: 0.52128, Mean batch Val AUC: 0.53295\n------------------------------\nEpoch 3, Train Loss: 0.508518, Mean batch Train Loss: 0.508518, AUC: 0.98973, Mean batch Train AUC: 0.98923,\n    Val Loss: 0.681388, Mean batch Val Loss: 0.681388, Val AUC: 0.55682, Mean batch Val AUC: 0.57112\n------------------------------\nEpoch 4, Train Loss: 0.441114, Mean batch Train Loss: 0.441114, AUC: 0.99554, Mean batch Train AUC: 0.99460,\n    Val Loss: 0.673574, Mean batch Val Loss: 0.673574, Val AUC: 0.58578, Mean batch Val AUC: 0.60539\n------------------------------\nEpoch 5, Train Loss: 0.368985, Mean batch Train Loss: 0.368985, AUC: 0.99868, Mean batch Train AUC: 0.99861,\n    Val Loss: 0.667929, Mean batch Val Loss: 0.667929, Val AUC: 0.61167, Mean batch Val AUC: 0.62760\n------------------------------\nEpoch 6, Train Loss: 0.306646, Mean batch Train Loss: 0.306646, AUC: 0.99956, Mean batch Train AUC: 1.00000,\n    Val Loss: 0.664882, Mean batch Val Loss: 0.664882, Val AUC: 0.62835, Mean batch Val AUC: 0.63807\n------------------------------\nEpoch 7, Train Loss: 0.249700, Mean batch Train Loss: 0.249700, AUC: 1.00000, Mean batch Train AUC: 1.00000,\n    Val Loss: 0.666024, Mean batch Val Loss: 0.666024, Val AUC: 0.63756, Mean batch Val AUC: 0.64217\n------------------------------\nEpoch 8, Train Loss: 0.195906, Mean batch Train Loss: 0.195906, AUC: 1.00000, Mean batch Train AUC: 1.00000,\n    Val Loss: 0.671024, Mean batch Val Loss: 0.671024, Val AUC: 0.64063, Mean batch Val AUC: 0.64618\n------------------------------\nEpoch 9, Train Loss: 0.151549, Mean batch Train Loss: 0.151549, AUC: 1.00000, Mean batch Train AUC: 1.00000,\n    Val Loss: 0.679804, Mean batch Val Loss: 0.679804, Val AUC: 0.64458, Mean batch Val AUC: 0.64464\n------------------------------\nEpoch 10, Train Loss: 0.111988, Mean batch Train Loss: 0.111988, AUC: 1.00000, Mean batch Train AUC: 1.00000,\n    Val Loss: 0.695000, Mean batch Val Loss: 0.695000, Val AUC: 0.64283, Mean batch Val AUC: 0.64751\n------------------------------\n\n\nI trained the model for 10 epochs and my loss is decreasing and AUC of train data became 1(overfit). But some times it may not overfit to the model. If it is not overfitting, there may be so many reasons like code written to create the model is incorrect, the model is not capable of learning the data, learning problems like vanishing or exploding gradients and many more. I will discuss these problems below and These problems may occur even while training with total data.\n\nCheck whether forward propagation is correct or not\nwhile training NN, we will use the vectorizing implementations of data manipulation. If we did any mistake in these implementations, our training process will give bad results. We can verify this with a simple hack using back prop dependency. Below are the steps to do.\n\nTake a few data points. Here I am taking 5 data points. You can get it from the data or you can generate random data with the same shape.\ndo forward propagation on the model we created with the above batch data.\nwrite a loss function that takes the true values, predicted values and returns loss as sum of the i^th data point output where i less than 5. I am using 3.\ndo the back prop and check the gradients with respect to the input data points. If you are getting non zero gradients only for i-th data point, your forward propagation is right otherwise, there is some error in the forward propagation and you have to debug the code to check the error.\n\nIn the implementation below, I have written basic implementation, not included any tensorboard/metrics and there is no need for those as well.\n\nNote: Gradient won’t flow through the embedding layer so you will get None gradients if you calculate the gradient with of loss with respect to the input. If you have the embedding layer at starting, please remove the embedding layer and give the input directly to the next layer. It is very easy to do because This layer can only be used as the first layer in a model.\n\n\n##same model with name changes and without emedding layer.\ndef get_model_check():\n    ##directly using embedding dimention of 1. It is only for checking so no problem with it. \n    input_layer = Input(shape=(24, 1), batch_size=10, name=\"input_layer_debug\")\n    \n    ##i am initilizing randomly. But you can use predefined embeddings. \n    #x_embedd = Embedding(input_dim=13732, output_dim=100, input_length=24, mask_zero=True, \n                        #embeddings_initializer=tf.keras.initializers.RandomNormal(mean=0, stddev=1, seed=23),\n                         #name=\"Embedding_layer\")(input_layer)\n    \n    x_lstm = LSTM(units=20, activation='tanh', recurrent_activation='sigmoid', use_bias=True, \n                 kernel_initializer=tf.keras.initializers.glorot_uniform(seed=26),\n                 recurrent_initializer=tf.keras.initializers.orthogonal(seed=54),\n                 bias_initializer=tf.keras.initializers.zeros(), name=\"LSTM_layer_debug\")(input_layer)\n    \n    x_out = Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.glorot_uniform(seed=45),\n                  name=\"output_layer_debug\")(x_lstm)\n    \n    basic_model_debug = Model(inputs=input_layer, outputs=x_out, name=\"basic_lstm_model_debug\")\n    \n    return basic_model_debug\n\nbasic_model_debug = get_model_check()\n\n##generated random 5 data points of shape (24,1) i.e 4 time steps and 1 dim embedding. \ntemp_features = np.random.randint(low=1, high=5, size=(5,24, 1))\n\n##generated the a random output zero or 1. I think, there is no use for this as well because \n#we will calculate loss only with predicted values\ntemp_outs = np.random.randint(0, 2, size=(5,1))\n\ndef loss_to_ckgrads(y_true, y_pred):\n    #y_pred is one dimentional you can give directly one data point prediction as loss. \n    #I am giving loss as 3rd data point prediction so we will get non zero gradients only for 3rd data point. \n    #if your prediction is sequence, please add all the i-th data point predictions and return those. \n    return y_pred[2]\n\ndef get_gradient(model, x_tensor):\n    #taping the gradients\n    with tf.GradientTape() as tape:\n        #explicitly telling to watch for input vector. it won't watch with repect to any inputs by default.\n        #it only watches the gradents with weight vectors\n        tape.watch(x_tensor)\n        #model predictions\n        preds = model(x_tensor)\n        #getting the loss\n        loss = loss_to_ckgrads(temp_outs, preds)\n    #getting the gradients    \n    grads = tape.gradient(loss, x_tensor)\n    return grads\n##making temp_feature as varible. We can get the gradients only if it is a varible so chnaging it to variable\ntemp_features = tf.Variable(tf.convert_to_tensor(temp_features, dtype=tf.float32))\n##\ngrads = get_gradient(basic_model_debug, temp_features)\nfor i in grads:\n    #checking whether all zeros or not\n    #except 3rd all the grdients should be zero i.e True\n    print(all(i==0))\n\nTrue\nTrue\nFalse\nTrue\nTrue\n\n\n If you are not getting all true except the i-th one, you may have any issue in your code. You have to check that and resolve it. Without that, don’t go to another step.\n\n\nWhat to do when Loss Explodes\nwhile training NN, you may get NaN/inf loss becuase of large or small values. Below are some causes\n\nNumerical stability issues.\n\nCheck the multiplications, if you are multiplying so many tensors at once, apply log and make it to addition.\nCheck for the division operation. any zero division is happening or not. Try to add a small constant like 1e-12 to the denominator.\nCheck the softmax function. If your vocab size if very large, try not to use the softmax function. calculate the loss based on the logits.\n\nIf the updates to the weights are very large, you may get numerical instability and it may explode.\n\nCheck for the Learning rate. If the learning rate is high, you may get this problem as well.\nCheck for the exploding gradient problem. In tensorboard, you can visualize the gradient histograms and you can check the problem. If gradients are exploding, try to clip the gradients. You can apply tf.linalg.normalize or tf.clip_by_value to your gradients after getting gradients from the GradientTape.\n\nIt may occur because of a poor choice of loss function i.e. allowing the calculation of large error values.\nIt may occur because of the poor data preparation i.e. allowing large differences in the target variables.\n\n\n\nWhat to do when loss Increases\nwhile training NN, our loss may increase some times. Below are some causes\n\nCheck for the Learning rate. If the learning rate is high, you may get this problem as well.\nCheck for the wrong loss function. Especially sign of the loss function.\nActivation functions applying over wrong dimensions. (you can find this out using the point number 1(checking forward propagation is correct or not)\n\n\n\nWhat to do when loss Oscillate\nwhile training NN, our loss may oscillate. Below are some causes\n\nCheck for the Learning rate. If the learning rate is high, you may get this problem as well.\nSometimes it may occur because of the exploding gradient problem. so check for that one as well. You can check this using the Tensorboard.\nIt may occur due to data pairing issues/data corruption. We already discussed this. so make sure to get the proper data.\n\n\n\nWhat to do when loss is constant\nwhile training NN, our loss constant. Below are some causes\n\nIf the updates to the weights are very low, you may end up in the same position.\n\nCheck for the learning rate. If the learning rate is low, our weights won’t update much so you may get this problem.\nCheck for Vanishing Gradient problem. In Tensorboard, you can visualize the gradient histograms and you can check the problem.\n\nYou can solve this by changing the activations to relu/leaky relu.\nYou can add skip connections to an easier flow of gradients.\nIf you have long sequences in RNN, you can divide into smaller ones and train with stateful LSTM’s(Truncated Back prop)\nBetter weight initialization may reduce this.\n\n\nToo much regularization may also cause this.\nIf you are using Relu activation, it may occur due to the dead neurons.\nIncorrect inputs to the loss function. I already discussed this while discussing the loss functions.\n\n\n\nWhat if we get memory Errors\nwhile training NN, many people face the memory exhaust errors because of the computing constraints.\n\nIf you are getting GPU memory exhaust error, try to reduce the batch size and train the neural network.\nIf your data doesn’t fit into the RAM you have, Try to create a data pipeline using tf.data or Keras/Python Data Generators and load the batchwise data. My personal choice is to use tf.data pipelines. Please check this blog to know more about it.\nPlease try to check for the duplicate operations like creating multiple models, storing temporary variables in the GPU memory.\n\n\n\nWhat if we Underfit to the data\nsome suggestions to make in decreasing order of priority\n\nMake your model bigger\nReduce/Remove regularization(L1/L2/Dropout) if any.\nDo error analysis. based on this try to change the preprocessing/data if needed.\nRead technical papers and choose the state of the art models.\nTune hyperparameters\nAdd custom features if needed.\n\n\n\nWhat if we Overfit to the data\nsome suggestions to make in decreasing order of priority\n\nAdd more training data\nAdd normalization layers(BN, layer norm)\nAdd data augmentation\nIncrease regularization\nDo error analysis. based on this try to change the preprocessing/data if needed.\nChoose a different model\nTune hyperparameters\n\n You can check some of my other blogs at this link. This is my LinkedIn and GitHub"
  },
  {
    "objectID": "notebooks/Logistic Regression summary-Uday.html",
    "href": "notebooks/Logistic Regression summary-Uday.html",
    "title": "Summary - Logistic Regression Algorithm",
    "section": "",
    "text": "\\(N\\) - Number of data points\n\\(X_q\\) - query data points\n\\(X_{qn}\\) - nth query data point\n\\(X\\) - input train data\n\\(D\\) - dimensionality of data\n\\(C\\) - Number of classes\n\\(C_i\\) - i^th class\n\\(N_k\\) - K nearst neighbors\n\\(m\\) - Number of epochs in SGD\nThis blog was originally published in this link."
  },
  {
    "objectID": "notebooks/Logistic Regression summary-Uday.html#terminology",
    "href": "notebooks/Logistic Regression summary-Uday.html#terminology",
    "title": "Summary - Logistic Regression Algorithm",
    "section": "",
    "text": "\\(N\\) - Number of data points\n\\(X_q\\) - query data points\n\\(X_{qn}\\) - nth query data point\n\\(X\\) - input train data\n\\(D\\) - dimensionality of data\n\\(C\\) - Number of classes\n\\(C_i\\) - i^th class\n\\(N_k\\) - K nearst neighbors\n\\(m\\) - Number of epochs in SGD\nThis blog was originally published in this link."
  },
  {
    "objectID": "notebooks/Logistic Regression summary-Uday.html#algorithm",
    "href": "notebooks/Logistic Regression summary-Uday.html#algorithm",
    "title": "Summary - Logistic Regression Algorithm",
    "section": "Algorithm",
    "text": "Algorithm\nWe have to optimize the log loss value. We can do this using the Gradient Descent. optimization problem is\n\\[\\begin{align}\n\\min_{W, b} \\sum_{i=1}^N \\log(e^{- y_i (X_i^T W + b)} + 1)\\\\\n\\text{ here }  C = 2 \\text{ and } y_i = +1 \\text{ or } -1\n\\end{align}\\]\n\nonce after optimization, we will get the W, b where loss \\(L(W,b)\\) is minimum. We can predict the class probability of a query point using W, b as below.\n\\[\\begin{align}\nP(y=1 | X_{qn}, W, b) = \\frac{1}{1+e^{-(WX_{qn}+b)}}\n\\end{align}\\]\nWe can re-write the above same loss formulation as below,\n\\[\\begin{align}\n\\min_{W_{1},W_{2}...W_{C}, b}- \\frac{1}{N} \\sum_{i=0}^{N-1} \\sum_{C=0}^{C-1} y_{i,c} \\log p_{i,c}\\\\\ny_{i,c} = 1 \\text{if sample i has label C taken from a set of C labels else 0} \\\\\np_{i,c} = \\frac{1}{1+e^{-(W_{C}X_{i}+b)}}\n\\end{align}\\]\nOnce after calculating W vectors, we can get C probabilities using \\(p_{i, c}\\) and then classify the given query point as maximum probability class.\n\nWe can write the final formulation with regularization as below\n\\[\\begin{align}\n\\min_{W, b} C * \\sum_{i=1}^N L(W,b) + \\text{regularization term}\n\\end{align}\\]\n\\[\\text{or}\\]\n\\[\\begin{align}\n\\min_{W, b}  \\sum_{i=1}^N L(W,b) + \\lambda * \\text{regularization term}\n\\end{align}\\] \nWe can handle multi-class classification in 3 ways, 1. Using one-vs-rest approach and first formulation of loss 2. Using one-vs-one approach and first formulation of loss 3. Multinomial logistic regression - 2nd formulation of loss and softmax function in the place of sigmoid function to get C class probabilities.\n\nYou can read more about LR in this blog and this blog"
  },
  {
    "objectID": "notebooks/Logistic Regression summary-Uday.html#useful-points-to-know",
    "href": "notebooks/Logistic Regression summary-Uday.html#useful-points-to-know",
    "title": "Summary - Logistic Regression Algorithm",
    "section": "Useful points to know",
    "text": "Useful points to know\n\nIf we want to use logistic regression for multi-class classification, we have to use one-vs-rest/one-vs-one/multinomial.\nTime complexity of training is \\(O(mCND)\\) and prediction is \\(O(CD)\\). Prediction time is very less so we can use logistic regression for low latency applications.\nWe can train the model using the iterative algorithm so there is no need to load total data into RAM (We can load chunk by chunk if we have huge data). We can use this model in online training too. Check the partial_fit function in sklearn implementation.\nIt assumes\n\nObservations should not come from repeated measurements.\nNone or Little Multicollinearity\nThe linearity of independent variables and Log odds - It requires features that are linearly related to the log-odds/logits, i.e \\(log(\\frac{P}{1-P})\\)\n\nWe can parallelize the multi-class logistic regression using one-vs-rest in sklearn. If you want for binary classification, use data parallelization, and accumulate gradients.\nIt is a linear model, it cannot classify non-linear data. If you have non-linear data, do feature engineering and try to get linear data.\nHigher dimensional data (not Huge) may lead to linear separable sometimes.\nAn increase in the feature value always leads to an increase or decrease in the target outcome(not probability, to logit/log-odds) so, it is a monotone model to log-odds/logits. Please check the interpretability below to know more about logit.\nIt won’t consider the interaction between the features. We have to create the interaction features if we need it. More interaction features may lead to less interpretability."
  },
  {
    "objectID": "notebooks/Logistic Regression summary-Uday.html#hyperparameters",
    "href": "notebooks/Logistic Regression summary-Uday.html#hyperparameters",
    "title": "Summary - Logistic Regression Algorithm",
    "section": "Hyperparameters",
    "text": "Hyperparameters\n\nC is the main hyperparameter we have (this is different from C classes we are using)\nHigh C value means less regularization strength.\nC is a multiplicative constant to loss term. While optimization we have to find the minimum value of the loss(Log loss+regularization). If we increase the C value, Log loss has to decrease so that final loss will decrease, i.e log loss will get very near to zero if C increases, so it may be overly certain about training data, so it overfits.\nThe below image contains decision hyperplanes for each value of C. You can observe that increasing the C value is reducing the regularization and overfitting to the data.\n\n\n\n\n\nHyperplane\n\n\n\n\nIn the above figure, we are getting similar classification results for all C greater than 0.5 but, increasing the C value reduces the regularization strength and increases the magnitude of weight vector values. You can check that in the below figure.\n\n\n\n\nweightvector\n\n\n\nIf two hyperplanes are giving similar results, get the best hyperplane based on the magnitude of weights. low weight is better because it gives more regularization and it won’t overestimate the probability value.\n\n\nImportant: Let’s take two planes. plane1 is 1.5f1 + 2.4f2 + 1 = 0, plane2 is 4.5f1 + 7.2f2 + 3 = 0. Both planes are mathematically same, only weight values are changed, but for a query point (0, 0), plane1 will give a probability of 0.73 and plane2 will give a probability of 0.95. But, the distance of the point (0,0) from both planes is the same that is 0.35 only. so large weights lead to overestimation of probabilities even though they are near to the plane."
  },
  {
    "objectID": "notebooks/Logistic Regression summary-Uday.html#interpretability",
    "href": "notebooks/Logistic Regression summary-Uday.html#interpretability",
    "title": "Summary - Logistic Regression Algorithm",
    "section": "Interpretability",
    "text": "Interpretability\nFrom logistic regression prediction probability function, we can write\n\n\\[\\begin{align}\nlog\\left(\\frac{P(y=1)}{1-P(y=1)}\\right)=log\\left(\\frac{P(y=1)}{P(y=0)}\\right)=W_{0}+W_{1}x_{1}+\\ldots+W_{d}x_{d}\n\\end{align}\\]\n\n\\[\\begin{align}\nodds = \\left(\\frac{P(y=1)}{1-P(y=1)}\\right)=\\left(\\frac{P(y=1)}{P(y=0)}\\right)=e^{W_{0}+W_{1}x_{1}+\\ldots+W_{d}x_{d}}\n\\end{align}\\]\n\nIf odds = 3 then \\(P(y=1)\\) is thrice as high as \\(P(y=0)\\)\n\nIf the feature \\(x_{j}\\) is changed by n unit and the ratio of odds after the change and before the change is\n\\[\\begin{align}\n\\frac{odds_{x_j+n}}{odds}=\\frac{e^{\\left(W_{0}+W{1}x_{1}+\\ldots+W_{j}(x_{j}+n)+\\ldots+W_{d}x_{d}\\right)}}{e^{\\left(W_{0}+W_{1}x_{1}+\\ldots+W_{j}x_{j}+\\ldots+W_{d}x_{d}\\right)}}\n\\end{align}\\]\n\n\\[\\begin{align}\n\\frac{odds_{x_j+1}}{odds}=e^{\\left(W_{j}(x_{j}+n)-W_{j}x_{j}\\right)}=e^{\\left(nW_j\\right)}\n\\end{align}\\]\n If we increase the feature \\(x_{j}\\) by n unit, the odds change by factor of \\(e^{nW_j}\\)\nWe can interpret the odds. If odds = k, \\(P(y=1) = k*P(y=0)\\)"
  },
  {
    "objectID": "notebooks/Logistic Regression summary-Uday.html#references",
    "href": "notebooks/Logistic Regression summary-Uday.html#references",
    "title": "Summary - Logistic Regression Algorithm",
    "section": "References",
    "text": "References\n\nhttps://stackexchange.com/\nAn Introduction to Statistical Learning - Book\nhttps://www.jeremyjordan.me/content/images/2017/06/Screen-Shot-2017-06-10-at-9.41.25-AM.png"
  }
]